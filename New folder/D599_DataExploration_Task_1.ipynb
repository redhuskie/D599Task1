{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce972983-14f7-4a43-98c8-87fb64956ae6",
   "metadata": {},
   "source": [
    "## WGU D599: Data Preparation and Exploration\n",
    "#### John D. Pickering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac240bc-a18e-4a3e-943f-757e55a1c7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import ast\n",
    "import numpy as np\n",
    "import shap\n",
    "import plotly\n",
    "from scipy.stats import zscore\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "from typing import Dict, List, Tuple, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44a3439-5602-4d72-9470-75273d748eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset into pandas as df\n",
    "df = pd.read_csv('Employee Turnover Dataset.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb733897-d248-496b-8b03-414f2dc80491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A1 - Identify the number of records and variables (columns)\n",
    "# Rows: 10199\n",
    "# Columns: 16\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e000b365-3e5d-4600-a013-00613ff98f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2 - List each variable and indicate the variable‚Äôs data type \n",
    "# (quantitative/numerical or qualitative/categorical) and data subtype (i.e., continuous/discrete or nominal/ordinal).\n",
    "def variable_type_summary(df):\n",
    "    summary = pd.DataFrame({\n",
    "        'Column': df.columns,\n",
    "        'Pandas_Dtype': df.dtypes.astype(str),\n",
    "        'Non_Null_Count': df.notnull().sum()\n",
    "    })\n",
    "\n",
    "    summary['Variable_Type'] = summary['Pandas_Dtype'].apply(lambda x:\n",
    "        'Quantitative' if 'int' in x or 'float' in x else\n",
    "        'Qualitative'\n",
    "    )\n",
    "\n",
    "    def guess_subtype(col):\n",
    "        if df[col].dtype in ['int64', 'float64']:\n",
    "            unique_vals = df[col].dropna().unique()\n",
    "            if df[col].dtype == 'int64' and len(unique_vals) < 20:\n",
    "                return 'Discrete'\n",
    "            else:\n",
    "                return 'Continuous'\n",
    "        elif df[col].dtype == 'object' or df[col].dtype.name == 'category':\n",
    "            n_unique = df[col].nunique()\n",
    "            if n_unique < 10:\n",
    "                unique_vals = df[col].dropna().unique()\n",
    "                return 'Ordinal' if sorted(unique_vals) == list(unique_vals) else 'Nominal'\n",
    "            else:\n",
    "                return 'Nominal'\n",
    "        return 'Unknown'\n",
    "\n",
    "    summary['Subtype'] = summary['Column'].apply(guess_subtype)\n",
    "\n",
    "    return summary[['Column', 'Pandas_Dtype', 'Variable_Type', 'Subtype']]\n",
    "\n",
    "summary_table = variable_type_summary(df)\n",
    "summary_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d63af9-e365-4217-9ef8-10a1347f95a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A3 - Identify a sample of observable values for each variable.\n",
    "df.head(5).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090ac463-e672-4f36-a984-e0cd83a8a6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B1 - Explain how you inspected the dataset to detect the following data quality issues: \n",
    "# Get total rows of duplicated data\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3184fb9-d431-419c-9004-fde5c72c8c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B1 - Show duplicated data\n",
    "df[df.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecf5ce5-d52f-406d-ad5c-b96cd3312eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_for_column(df, column_name):\n",
    "    if column_name not in df.columns:\n",
    "        return f\"Column '{column_name}' not found in DataFrame.\"\n",
    "    \n",
    "    total_rows = len(df)\n",
    "    missing_count = df[column_name].isna().sum()\n",
    "    missing_percent = round((missing_count / total_rows) * 100, 2)\n",
    "    \n",
    "    return {\n",
    "        'Column': column_name,\n",
    "        'Missing Count': missing_count,\n",
    "        'Missing %': f\"{missing_percent}%\"\n",
    "    }\n",
    "check_missing_for_column(df, 'EmployeeNumber')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576de35b-ae48-461b-b820-08edce62425d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - EmployeeNumber\n",
    "# Check first 5 rows of data\n",
    "df['EmployeeNumber'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865c6ffb-6a51-4e30-9ddc-fe60ebacb0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b766e8d-e4fd-4a9d-9b13-beee7e1d7ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B1 - Explain how you inspected the dataset to detect the following data quality issues: \n",
    "# missing values\n",
    "# Get the number of missing values p/comlumn\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d440b6-6693-4100-a6f2-f677f8eea36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B1 - missing values \n",
    "# Show percent of values missing by column.\n",
    "df.isna().mean() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c3509a-81c0-4b63-8512-84cc3396152f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B1 - inconsistent entries\n",
    "#  list all unique values in each categorical column\n",
    "for col in df.select_dtypes(include='object'):\n",
    "    print(f\"{col}:\", df[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d794ee63-6dae-446d-a7ad-f2ce5c5f5425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B1 - inconsistent entries\n",
    "# Find rare categories\n",
    "def find_rare_categories(df, column, threshold=10):\n",
    "    value_counts = df[column].value_counts(dropna=False)\n",
    "    rare = value_counts[value_counts < threshold]\n",
    "    return rare.reset_index().rename(columns={'index': column, column: 'Count'})\n",
    "\n",
    "# Check rare JobRoleArea values\n",
    "rare_job_roles = find_rare_categories(df, 'JobRoleArea', threshold=10)\n",
    "print(rare_job_roles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cb7a0d-0957-4ed2-b293-aa3bc754125b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B1 - Formatting Errors\n",
    "# Check for data types to ensure each field is listed correctly\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40b6f8a-9d40-4f32-8a11-21b27b5aa108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B1 - Formatting issues\n",
    "# 1. Check for leading/trailing whitespace in string columns (before cleaning)\n",
    "string_columns = df.select_dtypes(include='object').columns\n",
    "for col in string_columns:\n",
    "    whitespace_issues = df[col].apply(lambda x: isinstance(x, str) and (x != x.strip()))\n",
    "    if whitespace_issues.any():\n",
    "        print(f\"Column '{col}' has entries with leading/trailing whitespace.\")\n",
    "\n",
    "# 2. Check for inconsistent casing\n",
    "for col in string_columns:\n",
    "    unique_vals = df[col].dropna().unique()\n",
    "    if any(v != v.title() for v in unique_vals if isinstance(v, str)):\n",
    "        print(f\"Column '{col}' has inconsistent casing:\")\n",
    "        print(pd.Series(unique_vals))\n",
    "\n",
    "# 3. Check for special characters or formatting symbols in string columns\n",
    "import re\n",
    "for col in string_columns:\n",
    "    if df[col].astype(str).str.contains(r'[\\$%#@!&*]', regex=True).any():\n",
    "        print(f\"Column '{col}' contains special characters.\")\n",
    "\n",
    "# 4. Check for unexpected numeric types stored as objects\n",
    "for col in string_columns:\n",
    "    sample = df[col].dropna().sample(n=min(100, df[col].dropna().shape[0]), random_state=1)\n",
    "    if sample.apply(lambda x: str(x).replace('.', '', 1).isdigit()).mean() > 0.8:\n",
    "        print(f\"Column '{col}' may be numeric but stored as object.\")\n",
    "\n",
    "# 5. Check for placeholder or dummy values (e.g., 'N/A', 'unknown', '-')\n",
    "placeholder_values = ['n/a', 'na', 'unknown', '-', '--', 'none', 'null']\n",
    "for col in string_columns:\n",
    "    found = df[col].astype(str).str.lower().isin(placeholder_values).sum()\n",
    "    if found > 0:\n",
    "        print(f\"Column '{col}' has {found} placeholder or dummy values.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580346bf-288e-4ee2-bdb3-9b9bd5c21a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = inspect_columns(df, df.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcfec45-4f09-44f9-830c-c915e977a255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "from typing import Dict, List, Tuple, Any\n",
    "\n",
    "def inspect_data_quality(df: pd.DataFrame, \n",
    "                        numeric_columns: List[str] = None,\n",
    "                        categorical_columns: List[str] = None,\n",
    "                        outlier_method: str = 'iqr',\n",
    "                        outlier_threshold: float = 1.5) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Comprehensive data quality inspection function that checks for:\n",
    "    - Duplicate entries\n",
    "    - Missing values\n",
    "    - Inconsistent entries\n",
    "    - Formatting errors\n",
    "    - Outliers\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe to inspect\n",
    "    numeric_columns : List[str], optional\n",
    "        List of numeric column names. If None, will auto-detect\n",
    "    categorical_columns : List[str], optional\n",
    "        List of categorical column names. If None, will auto-detect\n",
    "    outlier_method : str, default 'iqr'\n",
    "        Method for outlier detection ('iqr', 'zscore', 'modified_zscore')\n",
    "    outlier_threshold : float, default 1.5\n",
    "        Threshold for outlier detection\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Dict[str, Any] : Comprehensive report of data quality issues\n",
    "    \"\"\"\n",
    "    \n",
    "    report = {\n",
    "        'dataset_overview': {},\n",
    "        'duplicates': {},\n",
    "        'missing_values': {},\n",
    "        'inconsistent_entries': {},\n",
    "        'formatting_errors': {},\n",
    "        'outliers': {},\n",
    "        'summary': {}\n",
    "    }\n",
    "    \n",
    "    # Dataset Overview\n",
    "    report['dataset_overview'] = {\n",
    "        'total_rows': len(df),\n",
    "        'total_columns': len(df.columns),\n",
    "        'columns': list(df.columns),\n",
    "        'data_types': df.dtypes.to_dict()\n",
    "    }\n",
    "    \n",
    "    # Auto-detect column types if not specified\n",
    "    if numeric_columns is None:\n",
    "        numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if categorical_columns is None:\n",
    "        categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    # 1. DUPLICATE ENTRIES\n",
    "    print(\"üîç Checking for duplicate entries...\")\n",
    "    \n",
    "    # Full row duplicates\n",
    "    full_duplicates = df.duplicated()\n",
    "    report['duplicates']['full_row_duplicates'] = {\n",
    "        'count': full_duplicates.sum(),\n",
    "        'percentage': (full_duplicates.sum() / len(df)) * 100,\n",
    "        'duplicate_indices': df[full_duplicates].index.tolist()\n",
    "    }\n",
    "    \n",
    "    # Column-wise duplicate analysis\n",
    "    column_duplicates = {}\n",
    "    for col in df.columns:\n",
    "        col_dups = df[col].duplicated()\n",
    "        column_duplicates[col] = {\n",
    "            'count': col_dups.sum(),\n",
    "            'percentage': (col_dups.sum() / len(df)) * 100,\n",
    "            'unique_values': df[col].nunique(),\n",
    "            'unique_percentage': (df[col].nunique() / len(df)) * 100\n",
    "        }\n",
    "    \n",
    "    report['duplicates']['column_wise'] = column_duplicates\n",
    "    \n",
    "    # 2. MISSING VALUES\n",
    "    print(\"üîç Checking for missing values...\")\n",
    "    \n",
    "    missing_stats = {}\n",
    "    for col in df.columns:\n",
    "        missing_count = df[col].isnull().sum()\n",
    "        missing_stats[col] = {\n",
    "            'count': int(missing_count),\n",
    "            'percentage': (missing_count / len(df)) * 100,\n",
    "            'missing_indices': df[df[col].isnull()].index.tolist()\n",
    "        }\n",
    "        \n",
    "        # Check for different representations of missing values\n",
    "        if df[col].dtype == 'object':\n",
    "            potential_missing = df[col].isin(['', ' ', 'NULL', 'null', 'NaN', 'nan', 'N/A', 'n/a', 'None', 'none'])\n",
    "            if potential_missing.sum() > 0:\n",
    "                missing_stats[col]['potential_missing_representations'] = {\n",
    "                    'count': int(potential_missing.sum()),\n",
    "                    'values': df[potential_missing][col].value_counts().to_dict()\n",
    "                }\n",
    "    \n",
    "    report['missing_values'] = missing_stats\n",
    "    \n",
    "    # 3. INCONSISTENT ENTRIES\n",
    "    print(\"üîç Checking for inconsistent entries...\")\n",
    "    \n",
    "    inconsistency_report = {}\n",
    "    \n",
    "    for col in categorical_columns:\n",
    "        if col in df.columns:\n",
    "            inconsistencies = {}\n",
    "            \n",
    "            # Case variations\n",
    "            if df[col].dtype == 'object':\n",
    "                values = df[col].dropna().astype(str)\n",
    "                case_variations = {}\n",
    "                \n",
    "                # Group by lowercase to find case variations\n",
    "                lowercase_groups = values.str.lower().value_counts()\n",
    "                for lower_val in lowercase_groups.index:\n",
    "                    original_variations = values[values.str.lower() == lower_val].unique()\n",
    "                    if len(original_variations) > 1:\n",
    "                        case_variations[lower_val] = original_variations.tolist()\n",
    "                \n",
    "                if case_variations:\n",
    "                    inconsistencies['case_variations'] = case_variations\n",
    "                \n",
    "                # Whitespace issues\n",
    "                whitespace_issues = {}\n",
    "                for val in values.unique():\n",
    "                    if val != val.strip():\n",
    "                        whitespace_issues[val] = val.strip()\n",
    "                \n",
    "                if whitespace_issues:\n",
    "                    inconsistencies['whitespace_issues'] = whitespace_issues\n",
    "                \n",
    "                # Similar values (potential typos)\n",
    "                from difflib import SequenceMatcher\n",
    "                unique_vals = values.unique()\n",
    "                similar_pairs = []\n",
    "                \n",
    "                for i, val1 in enumerate(unique_vals):\n",
    "                    for val2 in unique_vals[i+1:]:\n",
    "                        similarity = SequenceMatcher(None, str(val1).lower(), str(val2).lower()).ratio()\n",
    "                        if 0.8 <= similarity < 1.0:  # High similarity but not identical\n",
    "                            similar_pairs.append({\n",
    "                                'value1': val1,\n",
    "                                'value2': val2,\n",
    "                                'similarity': similarity,\n",
    "                                'count1': (values == val1).sum(),\n",
    "                                'count2': (values == val2).sum()\n",
    "                            })\n",
    "                \n",
    "                if similar_pairs:\n",
    "                    inconsistencies['similar_values'] = similar_pairs\n",
    "            \n",
    "            if inconsistencies:\n",
    "                inconsistency_report[col] = inconsistencies\n",
    "    \n",
    "    report['inconsistent_entries'] = inconsistency_report\n",
    "    \n",
    "    # 4. FORMATTING ERRORS\n",
    "    print(\"üîç Checking for formatting errors...\")\n",
    "    \n",
    "    formatting_errors = {}\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_errors = {}\n",
    "        \n",
    "        if df[col].dtype == 'object':\n",
    "            values = df[col].dropna().astype(str)\n",
    "            \n",
    "            # Check for mixed data types in string columns\n",
    "            numeric_pattern = re.compile(r'^-?\\d+\\.?\\d*$')\n",
    "            date_pattern = re.compile(r'\\d{1,4}[-/]\\d{1,2}[-/]\\d{1,4}')\n",
    "            email_pattern = re.compile(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$')\n",
    "            \n",
    "            mixed_types = {\n",
    "                'numeric_like': values[values.str.match(numeric_pattern, na=False)].tolist(),\n",
    "                'date_like': values[values.str.match(date_pattern, na=False)].tolist(),\n",
    "                'email_like': values[values.str.match(email_pattern, na=False)].tolist()\n",
    "            }\n",
    "            \n",
    "            # Remove empty lists\n",
    "            mixed_types = {k: v for k, v in mixed_types.items() if v}\n",
    "            if mixed_types:\n",
    "                col_errors['mixed_data_types'] = mixed_types\n",
    "            \n",
    "            # Check for unusual characters or encoding issues\n",
    "            unusual_chars = []\n",
    "            for val in values.unique()[:100]:  # Check first 100 unique values\n",
    "                if any(ord(char) > 127 for char in str(val)):  # Non-ASCII characters\n",
    "                    unusual_chars.append(val)\n",
    "            \n",
    "            if unusual_chars:\n",
    "                col_errors['unusual_characters'] = unusual_chars[:10]  # Show first 10\n",
    "        \n",
    "        # Check numeric columns stored as strings\n",
    "        elif col in numeric_columns and df[col].dtype == 'object':\n",
    "            non_numeric = df[~df[col].str.match(r'^-?\\d+\\.?\\d*$', na=False)][col].dropna()\n",
    "            if len(non_numeric) > 0:\n",
    "                col_errors['non_numeric_in_numeric_column'] = non_numeric.tolist()[:10]\n",
    "        \n",
    "        if col_errors:\n",
    "            formatting_errors[col] = col_errors\n",
    "    \n",
    "    report['formatting_errors'] = formatting_errors\n",
    "    \n",
    "    # 5. OUTLIERS\n",
    "    print(\"üîç Checking for outliers...\")\n",
    "    \n",
    "    outlier_report = {}\n",
    "    \n",
    "    for col in numeric_columns:\n",
    "        if col in df.columns and df[col].dtype in ['int64', 'float64']:\n",
    "            col_data = df[col].dropna()\n",
    "            \n",
    "            if len(col_data) == 0:\n",
    "                continue\n",
    "                \n",
    "            outliers = {}\n",
    "            \n",
    "            if outlier_method == 'iqr':\n",
    "                Q1 = col_data.quantile(0.25)\n",
    "                Q3 = col_data.quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - outlier_threshold * IQR\n",
    "                upper_bound = Q3 + outlier_threshold * IQR\n",
    "                \n",
    "                outlier_mask = (col_data < lower_bound) | (col_data > upper_bound)\n",
    "                outlier_values = col_data[outlier_mask]\n",
    "                \n",
    "                outliers['method'] = 'IQR'\n",
    "                outliers['bounds'] = {'lower': lower_bound, 'upper': upper_bound}\n",
    "                \n",
    "            elif outlier_method == 'zscore':\n",
    "                z_scores = np.abs((col_data - col_data.mean()) / col_data.std())\n",
    "                outlier_mask = z_scores > outlier_threshold\n",
    "                outlier_values = col_data[outlier_mask]\n",
    "                \n",
    "                outliers['method'] = 'Z-Score'\n",
    "                outliers['threshold'] = outlier_threshold\n",
    "                \n",
    "            elif outlier_method == 'modified_zscore':\n",
    "                median = col_data.median()\n",
    "                mad = np.median(np.abs(col_data - median))\n",
    "                modified_z_scores = 0.6745 * (col_data - median) / mad\n",
    "                outlier_mask = np.abs(modified_z_scores) > outlier_threshold\n",
    "                outlier_values = col_data[outlier_mask]\n",
    "                \n",
    "                outliers['method'] = 'Modified Z-Score'\n",
    "                outliers['threshold'] = outlier_threshold\n",
    "            \n",
    "            if len(outlier_values) > 0:\n",
    "                outliers.update({\n",
    "                    'count': len(outlier_values),\n",
    "                    'percentage': (len(outlier_values) / len(col_data)) * 100,\n",
    "                    'values': outlier_values.tolist(),\n",
    "                    'indices': outlier_values.index.tolist(),\n",
    "                    'statistics': {\n",
    "                        'min_outlier': outlier_values.min(),\n",
    "                        'max_outlier': outlier_values.max(),\n",
    "                        'mean_outlier': outlier_values.mean()\n",
    "                    }\n",
    "                })\n",
    "                \n",
    "                outlier_report[col] = outliers\n",
    "    \n",
    "    report['outliers'] = outlier_report\n",
    "    \n",
    "    # 6. SUMMARY\n",
    "    print(\"üìä Generating summary...\")\n",
    "    \n",
    "    total_issues = 0\n",
    "    issue_categories = []\n",
    "    \n",
    "    if report['duplicates']['full_row_duplicates']['count'] > 0:\n",
    "        total_issues += report['duplicates']['full_row_duplicates']['count']\n",
    "        issue_categories.append('duplicates')\n",
    "    \n",
    "    missing_issues = sum([stats['count'] for stats in report['missing_values'].values()])\n",
    "    if missing_issues > 0:\n",
    "        total_issues += missing_issues\n",
    "        issue_categories.append('missing_values')\n",
    "    \n",
    "    if report['inconsistent_entries']:\n",
    "        total_issues += len(report['inconsistent_entries'])\n",
    "        issue_categories.append('inconsistent_entries')\n",
    "    \n",
    "    if report['formatting_errors']:\n",
    "        total_issues += len(report['formatting_errors'])\n",
    "        issue_categories.append('formatting_errors')\n",
    "    \n",
    "    outlier_issues = sum([stats['count'] for stats in report['outliers'].values()])\n",
    "    if outlier_issues > 0:\n",
    "        total_issues += outlier_issues\n",
    "        issue_categories.append('outliers')\n",
    "    \n",
    "    report['summary'] = {\n",
    "        'total_issues_found': total_issues,\n",
    "        'issue_categories': issue_categories,\n",
    "        'data_quality_score': max(0, 100 - (total_issues / len(df)) * 100),\n",
    "        'recommendations': []\n",
    "    }\n",
    "    \n",
    "    # Add recommendations\n",
    "    recommendations = []\n",
    "    if report['duplicates']['full_row_duplicates']['count'] > 0:\n",
    "        recommendations.append(\"Remove duplicate rows to improve data integrity\")\n",
    "    if missing_issues > 0:\n",
    "        recommendations.append(\"Handle missing values through imputation or removal\")\n",
    "    if report['inconsistent_entries']:\n",
    "        recommendations.append(\"Standardize categorical values and fix case/whitespace issues\")\n",
    "    if report['formatting_errors']:\n",
    "        recommendations.append(\"Clean formatting errors and ensure consistent data types\")\n",
    "    if outlier_issues > 0:\n",
    "        recommendations.append(\"Investigate outliers - they may indicate data errors or genuine extreme values\")\n",
    "    \n",
    "    report['summary']['recommendations'] = recommendations\n",
    "    \n",
    "    print(\"‚úÖ Data quality inspection completed!\")\n",
    "    return report\n",
    "\n",
    "def print_quality_report(report: Dict[str, Any]) -> None:\n",
    "    \"\"\"\n",
    "    Print a formatted version of the data quality report\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"üìã DATA QUALITY INSPECTION REPORT\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Dataset Overview\n",
    "    print(\"\\nüìä DATASET OVERVIEW\")\n",
    "    print(\"-\" * 40)\n",
    "    overview = report['dataset_overview']\n",
    "    print(f\"Total Rows: {overview['total_rows']:,}\")\n",
    "    print(f\"Total Columns: {overview['total_columns']}\")\n",
    "    print(f\"Columns: {', '.join(overview['columns'])}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\nüéØ SUMMARY\")\n",
    "    print(\"-\" * 40)\n",
    "    summary = report['summary']\n",
    "    print(f\"Data Quality Score: {summary['data_quality_score']:.1f}/100\")\n",
    "    print(f\"Total Issues Found: {summary['total_issues_found']:,}\")\n",
    "    print(f\"Issue Categories: {', '.join(summary['issue_categories']) if summary['issue_categories'] else 'None'}\")\n",
    "    \n",
    "    # Recommendations\n",
    "    if summary['recommendations']:\n",
    "        print(\"\\nüí° RECOMMENDATIONS\")\n",
    "        print(\"-\" * 40)\n",
    "        for i, rec in enumerate(summary['recommendations'], 1):\n",
    "            print(f\"{i}. {rec}\")\n",
    "    \n",
    "    # Detailed findings\n",
    "    print(\"\\nüîç DETAILED FINDINGS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Duplicates\n",
    "    dup_count = report['duplicates']['full_row_duplicates']['count']\n",
    "    print(f\"Duplicate Rows: {dup_count:,} ({report['duplicates']['full_row_duplicates']['percentage']:.2f}%)\")\n",
    "    \n",
    "    # Missing Values\n",
    "    missing_cols = [col for col, stats in report['missing_values'].items() if stats['count'] > 0]\n",
    "    print(f\"Columns with Missing Values: {len(missing_cols)}\")\n",
    "    if missing_cols:\n",
    "        for col in missing_cols[:5]:  # Show top 5\n",
    "            stats = report['missing_values'][col]\n",
    "            print(f\"  ‚Ä¢ {col}: {stats['count']:,} missing ({stats['percentage']:.2f}%)\")\n",
    "    \n",
    "    # Inconsistencies\n",
    "    inconsistent_cols = len(report['inconsistent_entries'])\n",
    "    print(f\"Columns with Inconsistencies: {inconsistent_cols}\")\n",
    "    \n",
    "    # Formatting Errors\n",
    "    format_error_cols = len(report['formatting_errors'])\n",
    "    print(f\"Columns with Formatting Errors: {format_error_cols}\")\n",
    "    \n",
    "    # Outliers\n",
    "    outlier_cols = len(report['outliers'])\n",
    "    print(f\"Columns with Outliers: {outlier_cols}\")\n",
    "    if outlier_cols > 0:\n",
    "        for col, stats in list(report['outliers'].items())[:3]:  # Show top 3\n",
    "            print(f\"  ‚Ä¢ {col}: {stats['count']:,} outliers ({stats['percentage']:.2f}%)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your dataset\n",
    "    df = pd.read_csv('Employee Turnover Dataset.csv')\n",
    "    \n",
    "    # Define column types (adjust based on your data)\n",
    "    numeric_cols = ['EmployeeNumber', 'Age', 'Tenure', 'HoursWeekly', \n",
    "                   'AnnualSalary', 'DrivingCommuterDistance', \n",
    "                   'NumCompaniesPreviouslyWorked', 'AnnualProfessionalDevHrs']\n",
    "    \n",
    "    categorical_cols = ['Turnover', 'HourlyRate ', 'CompensationType', \n",
    "                       'JobRoleArea', 'Gender', 'MaritalStatus', \n",
    "                       'PaycheckMethod', 'TextMessageOptIn']\n",
    "    \n",
    "    # Run inspection\n",
    "    quality_report = inspect_data_quality(\n",
    "        df, \n",
    "        numeric_columns=numeric_cols,\n",
    "        categorical_columns=categorical_cols,\n",
    "        outlier_method='iqr',\n",
    "        outlier_threshold=1.5\n",
    "    )\n",
    "    \n",
    "    # Print formatted report\n",
    "    print_quality_report(quality_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04205964-d067-469f-909c-b2e720c87143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B1 - Formatting issues\n",
    "df['Gender'].value_counts()  # or compare .str.lower() vs .str.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1981b1-ef64-411a-9a46-b3ac9326854a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B1 - Outliers\n",
    "# Annual Salary - Look for outliers in an inv\n",
    "sns.boxplot(x=df['AnnualSalary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6edb2f-6908-4902-9526-b4313463b8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B1 - Outliers \n",
    "# Age\n",
    "sns.boxplot(x=df['Age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d622fd79-19c6-45fc-9595-1b655ad584f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B1 - Outliers\n",
    "# Driving Communter Distance\n",
    "sns.boxplot(x=df['DrivingCommuterDistance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86237b3a-0399-4029-b69c-e96b249b6d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B1 - Outliers\n",
    "# Annual Professional DevHrs\n",
    "sns.boxplot(x=df['AnnualProfessionalDevHrs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2ea8a5-751b-4f40-b3aa-5a92e43b8bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_scores = zscore(df['AnnualSalary'].dropna())\n",
    "outliers = df[(abs(z_scores) > 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f2286b-479d-44cd-9dbf-fff87630ca9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# C - Clean the data\n",
    "# ----------------------------------------\n",
    "\n",
    "# Step 0: Strip column names\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Step 1: Remove Duplicates\n",
    "df_cleaned = df.drop_duplicates().copy()\n",
    "\n",
    "# Step 2: Handle Missing Values\n",
    "if 'AnnualProfessionalDevHrs' in df_cleaned.columns:\n",
    "    median_dev_hours = df_cleaned['AnnualProfessionalDevHrs'].median()\n",
    "    df_cleaned.loc[:, 'AnnualProfessionalDevHrs'] = df_cleaned['AnnualProfessionalDevHrs'].fillna(median_dev_hours)\n",
    "\n",
    "# Step 3: Fix Inconsistent Entries\n",
    "if 'PaycheckMethod' in df_cleaned.columns:\n",
    "    df_cleaned.loc[:, 'PaycheckMethod'] = (\n",
    "        df_cleaned['PaycheckMethod']\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "        .replace({\n",
    "            'Mailed Check': 'Mail Check',\n",
    "            'Mail_Check': 'Mail Check',\n",
    "            'Mailedcheck': 'Mail Check',\n",
    "            'DirectDeposit': 'Direct Deposit',\n",
    "            'Direct_Deposit': 'Direct Deposit'\n",
    "        })\n",
    "    )\n",
    "\n",
    "if 'JobRoleArea' in df_cleaned.columns:\n",
    "    df_cleaned.loc[:, 'JobRoleArea'] = df_cleaned['JobRoleArea'].replace({\n",
    "        'InformationTechnology': 'Information Technology',\n",
    "        'Information_Technology': 'Information Technology',\n",
    "        'HumanResources': 'Human Resources',\n",
    "        'Human_Resources': 'Human Resources'\n",
    "    })\n",
    "\n",
    "text_columns = ['Gender', 'MaritalStatus', 'CompensationType', 'JobRoleArea', 'TextMessageOptIn', 'PaycheckMethod']\n",
    "for col in text_columns:\n",
    "    if col in df_cleaned.columns:\n",
    "        df_cleaned.loc[:, col] = df_cleaned[col].astype(str).str.strip().str.title()\n",
    "\n",
    "# Step 4: Fix Formatting\n",
    "if 'HourlyRate' in df_cleaned.columns and df_cleaned['HourlyRate'].dtype == 'object':\n",
    "    df_cleaned.loc[:, 'HourlyRate'] = (\n",
    "        df_cleaned['HourlyRate']\n",
    "        .astype(str)\n",
    "        .str.replace('$', '', regex=False)\n",
    "        .str.strip()\n",
    "        .astype(float)\n",
    "    )\n",
    "\n",
    "# Safely strip whitespace from all object-type string fields\n",
    "for col in df_cleaned.select_dtypes(include='object').columns:\n",
    "    df_cleaned.loc[:, col] = df_cleaned[col].astype(str).str.strip()\n",
    "\n",
    "# Step 5: Handle Outliers\n",
    "if 'AnnualSalary' in df_cleaned.columns:\n",
    "    Q1 = df_cleaned['AnnualSalary'].quantile(0.25)\n",
    "    Q3 = df_cleaned['AnnualSalary'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    df_cleaned.loc[:, 'AnnualSalary'] = df_cleaned['AnnualSalary'].apply(lambda x: min(x, upper_bound))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e588d09e-74bb-4145-96fe-5c5391ad6722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for unique values post cleaning\n",
    "for col in df_cleaned.select_dtypes(include='object'):\n",
    "    print(f\"{col}:\", df_cleaned[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86c5c5d-6816-48b3-a0e2-0ea1f1fe56e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export cleaned file\n",
    "df_cleaned.to_csv('Employee_Turnover_Cleaned.csv', index=False)\n",
    "print('Cleaned File exported')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (shap_env_new)",
   "language": "python",
   "name": "shap_env_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
