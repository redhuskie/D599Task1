{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e61827b-78ff-45c3-b9e0-9e181b1e0f84",
   "metadata": {},
   "source": [
    "## WGU D599: Data Preparation and Exploration\n",
    "#### John D. Pickering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "012e8546-9b55-4b94-a7a8-57b2feef7850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import ast\n",
    "import numpy as np\n",
    "import plotly\n",
    "from scipy.stats import zscore\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "from typing import Dict, List, Tuple, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "313599bb-7f1b-4950-b10f-771e42e65adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset into pandas as df\n",
    "df = pd.read_csv('Employee Turnover Dataset.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1eb8a0e1-b124-4fff-a9ac-90f2c2494fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10199 entries, 0 to 10198\n",
      "Data columns (total 16 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   EmployeeNumber                10199 non-null  int64  \n",
      " 1   Age                           10199 non-null  int64  \n",
      " 2   Tenure                        10199 non-null  int64  \n",
      " 3   Turnover                      10199 non-null  object \n",
      " 4   HourlyRate                    10199 non-null  object \n",
      " 5   HoursWeekly                   10199 non-null  int64  \n",
      " 6   CompensationType              10199 non-null  object \n",
      " 7   AnnualSalary                  10199 non-null  float64\n",
      " 8   DrivingCommuterDistance       10199 non-null  int64  \n",
      " 9   JobRoleArea                   10199 non-null  object \n",
      " 10  Gender                        10199 non-null  object \n",
      " 11  MaritalStatus                 10199 non-null  object \n",
      " 12  NumCompaniesPreviouslyWorked  9534 non-null   float64\n",
      " 13  AnnualProfessionalDevHrs      8230 non-null   float64\n",
      " 14  PaycheckMethod                10199 non-null  object \n",
      " 15  TextMessageOptIn              7933 non-null   object \n",
      "dtypes: float64(3), int64(5), object(8)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# A1 - Identify the number of records and variables (columns)\n",
    "# Rows: 10199\n",
    "# Columns: 16\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5511643a-6090-4aae-a2a4-fac906475639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column</th>\n",
       "      <th>Pandas_Dtype</th>\n",
       "      <th>Variable_Type</th>\n",
       "      <th>Subtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EmployeeNumber</th>\n",
       "      <td>EmployeeNumber</td>\n",
       "      <td>int64</td>\n",
       "      <td>Quantitative</td>\n",
       "      <td>Continuous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>Age</td>\n",
       "      <td>int64</td>\n",
       "      <td>Quantitative</td>\n",
       "      <td>Continuous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tenure</th>\n",
       "      <td>Tenure</td>\n",
       "      <td>int64</td>\n",
       "      <td>Quantitative</td>\n",
       "      <td>Continuous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Turnover</th>\n",
       "      <td>Turnover</td>\n",
       "      <td>object</td>\n",
       "      <td>Qualitative</td>\n",
       "      <td>Nominal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HourlyRate</th>\n",
       "      <td>HourlyRate</td>\n",
       "      <td>object</td>\n",
       "      <td>Qualitative</td>\n",
       "      <td>Nominal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HoursWeekly</th>\n",
       "      <td>HoursWeekly</td>\n",
       "      <td>int64</td>\n",
       "      <td>Quantitative</td>\n",
       "      <td>Discrete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CompensationType</th>\n",
       "      <td>CompensationType</td>\n",
       "      <td>object</td>\n",
       "      <td>Qualitative</td>\n",
       "      <td>Ordinal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AnnualSalary</th>\n",
       "      <td>AnnualSalary</td>\n",
       "      <td>float64</td>\n",
       "      <td>Quantitative</td>\n",
       "      <td>Continuous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DrivingCommuterDistance</th>\n",
       "      <td>DrivingCommuterDistance</td>\n",
       "      <td>int64</td>\n",
       "      <td>Quantitative</td>\n",
       "      <td>Continuous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JobRoleArea</th>\n",
       "      <td>JobRoleArea</td>\n",
       "      <td>object</td>\n",
       "      <td>Qualitative</td>\n",
       "      <td>Nominal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gender</th>\n",
       "      <td>Gender</td>\n",
       "      <td>object</td>\n",
       "      <td>Qualitative</td>\n",
       "      <td>Nominal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MaritalStatus</th>\n",
       "      <td>MaritalStatus</td>\n",
       "      <td>object</td>\n",
       "      <td>Qualitative</td>\n",
       "      <td>Nominal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NumCompaniesPreviouslyWorked</th>\n",
       "      <td>NumCompaniesPreviouslyWorked</td>\n",
       "      <td>float64</td>\n",
       "      <td>Quantitative</td>\n",
       "      <td>Continuous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AnnualProfessionalDevHrs</th>\n",
       "      <td>AnnualProfessionalDevHrs</td>\n",
       "      <td>float64</td>\n",
       "      <td>Quantitative</td>\n",
       "      <td>Continuous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PaycheckMethod</th>\n",
       "      <td>PaycheckMethod</td>\n",
       "      <td>object</td>\n",
       "      <td>Qualitative</td>\n",
       "      <td>Nominal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TextMessageOptIn</th>\n",
       "      <td>TextMessageOptIn</td>\n",
       "      <td>object</td>\n",
       "      <td>Qualitative</td>\n",
       "      <td>Nominal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Column Pandas_Dtype  \\\n",
       "EmployeeNumber                              EmployeeNumber        int64   \n",
       "Age                                                    Age        int64   \n",
       "Tenure                                              Tenure        int64   \n",
       "Turnover                                          Turnover       object   \n",
       "HourlyRate                                     HourlyRate        object   \n",
       "HoursWeekly                                    HoursWeekly        int64   \n",
       "CompensationType                          CompensationType       object   \n",
       "AnnualSalary                                  AnnualSalary      float64   \n",
       "DrivingCommuterDistance            DrivingCommuterDistance        int64   \n",
       "JobRoleArea                                    JobRoleArea       object   \n",
       "Gender                                              Gender       object   \n",
       "MaritalStatus                                MaritalStatus       object   \n",
       "NumCompaniesPreviouslyWorked  NumCompaniesPreviouslyWorked      float64   \n",
       "AnnualProfessionalDevHrs          AnnualProfessionalDevHrs      float64   \n",
       "PaycheckMethod                              PaycheckMethod       object   \n",
       "TextMessageOptIn                          TextMessageOptIn       object   \n",
       "\n",
       "                             Variable_Type     Subtype  \n",
       "EmployeeNumber                Quantitative  Continuous  \n",
       "Age                           Quantitative  Continuous  \n",
       "Tenure                        Quantitative  Continuous  \n",
       "Turnover                       Qualitative     Nominal  \n",
       "HourlyRate                     Qualitative     Nominal  \n",
       "HoursWeekly                   Quantitative    Discrete  \n",
       "CompensationType               Qualitative     Ordinal  \n",
       "AnnualSalary                  Quantitative  Continuous  \n",
       "DrivingCommuterDistance       Quantitative  Continuous  \n",
       "JobRoleArea                    Qualitative     Nominal  \n",
       "Gender                         Qualitative     Nominal  \n",
       "MaritalStatus                  Qualitative     Nominal  \n",
       "NumCompaniesPreviouslyWorked  Quantitative  Continuous  \n",
       "AnnualProfessionalDevHrs      Quantitative  Continuous  \n",
       "PaycheckMethod                 Qualitative     Nominal  \n",
       "TextMessageOptIn               Qualitative     Nominal  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A2 - List each variable and indicate the variableâ€™s data type \n",
    "# (quantitative/numerical or qualitative/categorical) and data subtype (i.e., continuous/discrete or nominal/ordinal).\n",
    "def variable_type_summary(df):\n",
    "    summary = pd.DataFrame({\n",
    "        'Column': df.columns,\n",
    "        'Pandas_Dtype': df.dtypes.astype(str),\n",
    "        'Non_Null_Count': df.notnull().sum()\n",
    "    })\n",
    "\n",
    "    summary['Variable_Type'] = summary['Pandas_Dtype'].apply(lambda x:\n",
    "        'Quantitative' if 'int' in x or 'float' in x else\n",
    "        'Qualitative'\n",
    "    )\n",
    "\n",
    "    def guess_subtype(col):\n",
    "        if df[col].dtype in ['int64', 'float64']:\n",
    "            unique_vals = df[col].dropna().unique()\n",
    "            if df[col].dtype == 'int64' and len(unique_vals) < 20:\n",
    "                return 'Discrete'\n",
    "            else:\n",
    "                return 'Continuous'\n",
    "        elif df[col].dtype == 'object' or df[col].dtype.name == 'category':\n",
    "            n_unique = df[col].nunique()\n",
    "            if n_unique < 10:\n",
    "                unique_vals = df[col].dropna().unique()\n",
    "                return 'Ordinal' if sorted(unique_vals) == list(unique_vals) else 'Nominal'\n",
    "            else:\n",
    "                return 'Nominal'\n",
    "        return 'Unknown'\n",
    "\n",
    "    summary['Subtype'] = summary['Column'].apply(guess_subtype)\n",
    "\n",
    "    return summary[['Column', 'Pandas_Dtype', 'Variable_Type', 'Subtype']]\n",
    "\n",
    "summary_table = variable_type_summary(df)\n",
    "summary_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fad39205-ab62-4b67-892a-f95c4aff0225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EmployeeNumber</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>28</td>\n",
       "      <td>33</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tenure</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Turnover</th>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HourlyRate</th>\n",
       "      <td>$24.37</td>\n",
       "      <td>$24.37</td>\n",
       "      <td>$22.52</td>\n",
       "      <td>$22.52</td>\n",
       "      <td>$88.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HoursWeekly</th>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CompensationType</th>\n",
       "      <td>Salary</td>\n",
       "      <td>Salary</td>\n",
       "      <td>Salary</td>\n",
       "      <td>Salary</td>\n",
       "      <td>Salary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AnnualSalary</th>\n",
       "      <td>50689.6</td>\n",
       "      <td>50689.6</td>\n",
       "      <td>46841.6</td>\n",
       "      <td>46841.6</td>\n",
       "      <td>284641.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DrivingCommuterDistance</th>\n",
       "      <td>89</td>\n",
       "      <td>89</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JobRoleArea</th>\n",
       "      <td>Research</td>\n",
       "      <td>Research</td>\n",
       "      <td>Information_Technology</td>\n",
       "      <td>Information_Technology</td>\n",
       "      <td>Sales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gender</th>\n",
       "      <td>Female</td>\n",
       "      <td>Female</td>\n",
       "      <td>Female</td>\n",
       "      <td>Female</td>\n",
       "      <td>Prefer Not to Answer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MaritalStatus</th>\n",
       "      <td>Married</td>\n",
       "      <td>Married</td>\n",
       "      <td>Single</td>\n",
       "      <td>Single</td>\n",
       "      <td>Single</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NumCompaniesPreviouslyWorked</th>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AnnualProfessionalDevHrs</th>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PaycheckMethod</th>\n",
       "      <td>Mail Check</td>\n",
       "      <td>Mail Check</td>\n",
       "      <td>Mailed Check</td>\n",
       "      <td>Mailed Check</td>\n",
       "      <td>Mail Check</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TextMessageOptIn</th>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       0           1                       2  \\\n",
       "EmployeeNumber                         1           2                       3   \n",
       "Age                                   28          33                      22   \n",
       "Tenure                                 6           2                       1   \n",
       "Turnover                             Yes         Yes                      No   \n",
       "HourlyRate                       $24.37      $24.37                  $22.52    \n",
       "HoursWeekly                           40          40                      40   \n",
       "CompensationType                  Salary      Salary                  Salary   \n",
       "AnnualSalary                     50689.6     50689.6                 46841.6   \n",
       "DrivingCommuterDistance               89          89                      35   \n",
       "JobRoleArea                     Research    Research  Information_Technology   \n",
       "Gender                            Female      Female                  Female   \n",
       "MaritalStatus                    Married     Married                  Single   \n",
       "NumCompaniesPreviouslyWorked         3.0         6.0                     1.0   \n",
       "AnnualProfessionalDevHrs             7.0         7.0                     8.0   \n",
       "PaycheckMethod                Mail Check  Mail Check            Mailed Check   \n",
       "TextMessageOptIn                     Yes         Yes                     Yes   \n",
       "\n",
       "                                                   3                     4  \n",
       "EmployeeNumber                                     4                     5  \n",
       "Age                                               23                    40  \n",
       "Tenure                                             1                     6  \n",
       "Turnover                                          No                    No  \n",
       "HourlyRate                                   $22.52                $88.77   \n",
       "HoursWeekly                                       40                    40  \n",
       "CompensationType                              Salary                Salary  \n",
       "AnnualSalary                                 46841.6              284641.6  \n",
       "DrivingCommuterDistance                           35                    12  \n",
       "JobRoleArea                   Information_Technology                 Sales  \n",
       "Gender                                        Female  Prefer Not to Answer  \n",
       "MaritalStatus                                 Single                Single  \n",
       "NumCompaniesPreviouslyWorked                     3.0                   7.0  \n",
       "AnnualProfessionalDevHrs                         NaN                   NaN  \n",
       "PaycheckMethod                          Mailed Check            Mail Check  \n",
       "TextMessageOptIn                                 Yes                   Yes  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A3 - Identify a sample of observable values for each variable.\n",
    "df.head(5).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3a0d580-0104-43e2-9a7b-5cdd85d035c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#B2 - Set Functions\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style for Jupyter\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "def inspect_data_quality(df: pd.DataFrame, \n",
    "                        numeric_columns: List[str] = None,\n",
    "                        categorical_columns: List[str] = None,\n",
    "                        outlier_method: str = 'iqr',\n",
    "                        outlier_threshold: float = 1.5,\n",
    "                        show_plots: bool = True) -> Dict[str, Any]:\n",
    "  \n",
    "    \n",
    "    print(\" STARTING DATA QUALITY INSPECTION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    report = {\n",
    "        'dataset_overview': {},\n",
    "        'duplicates': {},\n",
    "        'missing_values': {},\n",
    "        'inconsistent_entries': {},\n",
    "        'formatting_errors': {},\n",
    "        'outliers': {},\n",
    "        'summary': {}\n",
    "    }\n",
    "    \n",
    "    # Dataset Overview\n",
    "    report['dataset_overview'] = {\n",
    "        'total_rows': len(df),\n",
    "        'total_columns': len(df.columns),\n",
    "        'columns': list(df.columns),\n",
    "        'data_types': df.dtypes.to_dict(),\n",
    "        'memory_usage': df.memory_usage(deep=True).sum()\n",
    "    }\n",
    "    \n",
    "    # Detect column types\n",
    "    if numeric_columns is None:\n",
    "        numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if categorical_columns is None:\n",
    "        categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    print(f\" Dataset: {len(df):,} rows Ã— {len(df.columns)} columns\")\n",
    "    print(f\" Numeric columns: {len(numeric_columns)}\")\n",
    "    print(f\" Categorical columns: {len(categorical_columns)}\")\n",
    "    print()\n",
    "    \n",
    "    # 1. DUPLICATE ENTRIES\n",
    "    print(\"1. CHECKING DUPLICATE ENTRIES...\")\n",
    "    \n",
    "    # Full row duplicates\n",
    "    full_duplicates = df.duplicated()\n",
    "    duplicate_rows = df[full_duplicates]\n",
    "    \n",
    "    report['duplicates']['full_row_duplicates'] = {\n",
    "        'count': full_duplicates.sum(),\n",
    "        'percentage': (full_duplicates.sum() / len(df)) * 100,\n",
    "        'duplicate_indices': duplicate_rows.index.tolist()\n",
    "    }\n",
    "    \n",
    "    # Column-wise duplicate analysis\n",
    "    column_duplicates = {}\n",
    "    for col in df.columns:\n",
    "        col_dups = df[col].duplicated()\n",
    "        column_duplicates[col] = {\n",
    "            'count': col_dups.sum(),\n",
    "            'percentage': (col_dups.sum() / len(df)) * 100,\n",
    "            'unique_values': df[col].nunique(),\n",
    "            'unique_percentage': (df[col].nunique() / len(df)) * 100\n",
    "        }\n",
    "    \n",
    "    report['duplicates']['column_wise'] = column_duplicates\n",
    "    \n",
    "    print(f\"Full row duplicates: {full_duplicates.sum():,} ({(full_duplicates.sum() / len(df)) * 100:.2f}%)\")\n",
    "    \n",
    "    if show_plots and len(df.columns) <= 20:  # Only show if manageable number of columns\n",
    "        # Visualization: Uniqueness by column\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "        unique_percentages = [column_duplicates[col]['unique_percentage'] for col in df.columns]\n",
    "        \n",
    "        bars = ax.bar(range(len(df.columns)), unique_percentages, color='skyblue', alpha=0.7)\n",
    "        ax.set_xlabel('Columns')\n",
    "        ax.set_ylabel('Unique Values (%)')\n",
    "        ax.set_title('Uniqueness Percentage by Column')\n",
    "        ax.set_xticks(range(len(df.columns)))\n",
    "        ax.set_xticklabels(df.columns, rotation=45, ha='right')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, val in zip(bars, unique_percentages):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "                   f'{val:.1f}%', ha='center', va='bottom', fontsize=8)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # 2. MISSING VALUES\n",
    "    print(\"\\n2. CHECKING MISSING VALUES...\")\n",
    "    \n",
    "    missing_stats = {}\n",
    "    total_missing = 0\n",
    "    \n",
    "    for col in df.columns:\n",
    "        missing_count = df[col].isnull().sum()\n",
    "        missing_stats[col] = {\n",
    "            'count': int(missing_count),\n",
    "            'percentage': (missing_count / len(df)) * 100,\n",
    "            'missing_indices': df[df[col].isnull()].index.tolist()\n",
    "        }\n",
    "        total_missing += missing_count\n",
    "        \n",
    "        # Check for different representations of missing values\n",
    "        if df[col].dtype == 'object':\n",
    "            potential_missing = df[col].isin(['', ' ', 'NULL', 'null', 'NaN', 'nan', 'N/A', 'n/a', 'None', 'none'])\n",
    "            if potential_missing.sum() > 0:\n",
    "                missing_stats[col]['potential_missing_representations'] = {\n",
    "                    'count': int(potential_missing.sum()),\n",
    "                    'values': df[potential_missing][col].value_counts().to_dict()\n",
    "                }\n",
    "    \n",
    "    report['missing_values'] = missing_stats\n",
    "    print(f\"Total missing values: {total_missing:,} ({(total_missing / (len(df) * len(df.columns))) * 100:.2f}% of all data)\")\n",
    "    \n",
    "    # Show columns with missing values\n",
    "    missing_cols = [(col, stats['count'], stats['percentage']) \n",
    "                   for col, stats in missing_stats.items() if stats['count'] > 0]\n",
    "    if missing_cols:\n",
    "        print(\"  Columns with missing values:\")\n",
    "        for col, count, pct in sorted(missing_cols, key=lambda x: x[2], reverse=True)[:10]:\n",
    "            print(f\"      â€¢ {col}: {count:,} ({pct:.2f}%)\")\n",
    "    \n",
    "    if show_plots and missing_cols:\n",
    "        # Visualization: Missing values heatmap\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Missing values by column (bar chart)\n",
    "        cols, counts, pcts = zip(*missing_cols) if missing_cols else ([], [], [])\n",
    "        ax1.barh(range(len(cols)), pcts, color='coral', alpha=0.7)\n",
    "        ax1.set_yticks(range(len(cols)))\n",
    "        ax1.set_yticklabels(cols)\n",
    "        ax1.set_xlabel('Missing Values (%)')\n",
    "        ax1.set_title('Missing Values by Column')\n",
    "        \n",
    "        # Missing values heatmap (sample)\n",
    "        if len(df) <= 1000:  # Only for smaller datasets\n",
    "            missing_matrix = df.isnull()\n",
    "            sns.heatmap(missing_matrix.T, cbar=True, ax=ax2, cmap='RdYlBu_r')\n",
    "            ax2.set_title('Missing Values Pattern (Sample)')\n",
    "            ax2.set_xlabel('Row Index')\n",
    "        else:\n",
    "            # Sample for large datasets\n",
    "            sample_df = df.sample(min(1000, len(df)), random_state=42)\n",
    "            missing_matrix = sample_df.isnull()\n",
    "            sns.heatmap(missing_matrix.T, cbar=True, ax=ax2, cmap='RdYlBu_r')\n",
    "            ax2.set_title('Missing Values Pattern (Random Sample)')\n",
    "            ax2.set_xlabel('Sample Row Index')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # 3. INCONSISTENT ENTRIES\n",
    "    print(\"\\n 3. CHECKING INCONSISTENT ENTRIES...\")\n",
    "    \n",
    "    inconsistency_report = {}\n",
    "    total_inconsistencies = 0\n",
    "    \n",
    "    for col in categorical_columns:\n",
    "        if col in df.columns:\n",
    "            inconsistencies = {}\n",
    "            \n",
    "            # Case variations\n",
    "            if df[col].dtype == 'object':\n",
    "                values = df[col].dropna().astype(str)\n",
    "                case_variations = {}\n",
    "                \n",
    "                # Group by lowercase to find case variations\n",
    "                lowercase_groups = values.str.lower().value_counts()\n",
    "                for lower_val in lowercase_groups.index:\n",
    "                    original_variations = values[values.str.lower() == lower_val].unique()\n",
    "                    if len(original_variations) > 1:\n",
    "                        case_variations[lower_val] = original_variations.tolist()\n",
    "                \n",
    "                if case_variations:\n",
    "                    inconsistencies['case_variations'] = case_variations\n",
    "                    total_inconsistencies += len(case_variations)\n",
    "                \n",
    "                # Whitespace issues\n",
    "                whitespace_issues = {}\n",
    "                for val in values.unique():\n",
    "                    if val != val.strip():\n",
    "                        whitespace_issues[val] = val.strip()\n",
    "                \n",
    "                if whitespace_issues:\n",
    "                    inconsistencies['whitespace_issues'] = whitespace_issues\n",
    "                    total_inconsistencies += len(whitespace_issues)\n",
    "                \n",
    "                # Similar values and standardization issues\n",
    "                from difflib import SequenceMatcher\n",
    "                unique_vals = values.unique()\n",
    "                similar_pairs = []\n",
    "                \n",
    "                # Enhanced standardization issue detection\n",
    "                standardization_groups = {}\n",
    "                \n",
    "                # Limit comparison for performance\n",
    "                if len(unique_vals) <= 100:\n",
    "                    for i, val1 in enumerate(unique_vals):\n",
    "                        for val2 in unique_vals[i+1:]:\n",
    "                            if len(str(val1)) > 2 and len(str(val2)) > 2:  # Skip very short strings\n",
    "                                \n",
    "                                # Create normalized versions for comparison\n",
    "                                norm_val1 = str(val1).lower().replace('_', ' ').replace('-', ' ').strip()\n",
    "                                norm_val2 = str(val2).lower().replace('_', ' ').replace('-', ' ').strip()\n",
    "                                \n",
    "                                # Check for exact matches after normalization\n",
    "                                if norm_val1 == norm_val2 and str(val1) != str(val2):\n",
    "                                    # This is a standardization issue (same meaning, different format)\n",
    "                                    group_key = norm_val1\n",
    "                                    if group_key not in standardization_groups:\n",
    "                                        standardization_groups[group_key] = []\n",
    "                                    \n",
    "                                    # Add both values to the group if not already there\n",
    "                                    if val1 not in [item['value'] for item in standardization_groups[group_key]]:\n",
    "                                        standardization_groups[group_key].append({\n",
    "                                            'value': val1,\n",
    "                                            'count': int((values == val1).sum())\n",
    "                                        })\n",
    "                                    if val2 not in [item['value'] for item in standardization_groups[group_key]]:\n",
    "                                        standardization_groups[group_key].append({\n",
    "                                            'value': val2,\n",
    "                                            'count': int((values == val2).sum())\n",
    "                                        })\n",
    "                                \n",
    "                                else:\n",
    "                                    # Check for high similarity (potential typos)\n",
    "                                    similarity = SequenceMatcher(None, norm_val1, norm_val2).ratio()\n",
    "                                    if 0.8 <= similarity < 1.0:  # High similarity but not identical\n",
    "                                        similar_pairs.append({\n",
    "                                            'value1': val1,\n",
    "                                            'value2': val2,\n",
    "                                            'similarity': similarity,\n",
    "                                            'count1': int((values == val1).sum()),\n",
    "                                            'count2': int((values == val2).sum()),\n",
    "                                            'normalized1': norm_val1,\n",
    "                                            'normalized2': norm_val2\n",
    "                                        })\n",
    "                \n",
    "                # Store standardization issues\n",
    "                if standardization_groups:\n",
    "                    inconsistencies['standardization_issues'] = standardization_groups\n",
    "                    total_inconsistencies += len(standardization_groups)\n",
    "                    \n",
    "                    # Print detailed standardization issues\n",
    "                    print(f\"      ðŸŽ¯ Found {len(standardization_groups)} standardization groups in '{col}':\")\n",
    "                    for group_name, variants in list(standardization_groups.items())[:3]:  # Show first 3\n",
    "                        total_count = sum([item['count'] for item in variants])\n",
    "                        variant_strs = [f\"'{item['value']}' ({item['count']})\" for item in variants]\n",
    "                        print(f\"         â€¢ {group_name}: {', '.join(variant_strs)} | Total: {total_count}\")\n",
    "                \n",
    "                if similar_pairs:\n",
    "                    inconsistencies['similar_values'] = similar_pairs\n",
    "                    total_inconsistencies += len(similar_pairs)\n",
    "            \n",
    "            if inconsistencies:\n",
    "                inconsistency_report[col] = inconsistencies\n",
    "    \n",
    "    report['inconsistent_entries'] = inconsistency_report\n",
    "    print(f\"   âœ“ Inconsistency issues found: {total_inconsistencies}\")\n",
    "    \n",
    "    if inconsistency_report:\n",
    "        print(\"   ðŸ“‹ Columns with inconsistencies:\")\n",
    "        for col, issues in list(inconsistency_report.items())[:5]:\n",
    "            issue_types = list(issues.keys())\n",
    "            print(f\"      â€¢ {col}: {', '.join(issue_types)}\")\n",
    "            \n",
    "            # Show standardization issues summary\n",
    "            if 'standardization_issues' in issues:\n",
    "                std_issues = issues['standardization_issues']\n",
    "                affected_values = sum([len(variants) for variants in std_issues.values()])\n",
    "                print(f\"        â†’ {len(std_issues)} standardization groups affecting {affected_values} unique values\")\n",
    "    \n",
    "    # 4. FORMATTING ERRORS\n",
    "    print(\"\\nðŸ” 4. CHECKING FORMATTING ERRORS...\")\n",
    "    \n",
    "    formatting_errors = {}\n",
    "    total_format_errors = 0\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_errors = {}\n",
    "        \n",
    "        if df[col].dtype == 'object':\n",
    "            values = df[col].dropna().astype(str)\n",
    "            \n",
    "            # 1. Check for mixed data types in string columns\n",
    "            numeric_pattern = re.compile(r'^-?\\d+\\.?\\d*$')\n",
    "            date_pattern = re.compile(r'\\d{1,4}[-/]\\d{1,2}[-/]\\d{1,4}')\n",
    "            email_pattern = re.compile(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$')\n",
    "            \n",
    "            mixed_types = {}\n",
    "            \n",
    "            numeric_matches = values[values.str.match(numeric_pattern, na=False)]\n",
    "            if len(numeric_matches) > 0 and len(numeric_matches) < len(values):\n",
    "                mixed_types['numeric_like'] = numeric_matches.tolist()[:10]\n",
    "            \n",
    "            date_matches = values[values.str.match(date_pattern, na=False)]\n",
    "            if len(date_matches) > 0:\n",
    "                mixed_types['date_like'] = date_matches.tolist()[:10]\n",
    "            \n",
    "            email_matches = values[values.str.match(email_pattern, na=False)]\n",
    "            if len(email_matches) > 0:\n",
    "                mixed_types['email_like'] = email_matches.tolist()[:10]\n",
    "            \n",
    "            if mixed_types:\n",
    "                col_errors['mixed_data_types'] = mixed_types\n",
    "                total_format_errors += len(mixed_types)\n",
    "            \n",
    "            # 2. Check for inconsistent formatting within the same column\n",
    "            format_inconsistencies = {}\n",
    "            \n",
    "            # Currency formatting inconsistencies\n",
    "            if any(keyword in col.lower() for keyword in ['salary', 'rate', 'pay', 'wage', 'cost', 'price']):\n",
    "                currency_patterns = {\n",
    "                    'dollar_sign_prefix': values.str.match(r'^\\$[\\d,]+\\.?\\d*$', na=False).sum(),\n",
    "                    'dollar_sign_suffix': values.str.match(r'^[\\d,]+\\.?\\d*\\$$', na=False).sum(),\n",
    "                    'no_dollar_sign': values.str.match(r'^[\\d,]+\\.?\\d*$', na=False).sum(),\n",
    "                    'with_commas': values.str.contains(',', na=False).sum(),\n",
    "                    'without_commas': (~values.str.contains(',', na=False)).sum()\n",
    "                }\n",
    "                # If multiple formats exist, it's inconsistent\n",
    "                non_zero_formats = sum(1 for count in currency_patterns.values() if count > 0)\n",
    "                if non_zero_formats > 1:\n",
    "                    format_inconsistencies['currency_formatting'] = currency_patterns\n",
    "            \n",
    "            # Phone number formatting inconsistencies\n",
    "            if any(keyword in col.lower() for keyword in ['phone', 'tel', 'mobile', 'contact']):\n",
    "                phone_patterns = {\n",
    "                    'with_dashes': values.str.match(r'^\\d{3}-\\d{3}-\\d{4}$', na=False).sum(),\n",
    "                    'with_parentheses': values.str.match(r'^\\(\\d{3}\\)\\s?\\d{3}-\\d{4}$', na=False).sum(),\n",
    "                    'dots': values.str.match(r'^\\d{3}\\.\\d{3}\\.\\d{4}$', na=False).sum(),\n",
    "                    'no_formatting': values.str.match(r'^\\d{10}$', na=False).sum()\n",
    "                }\n",
    "                non_zero_formats = sum(1 for count in phone_patterns.values() if count > 0)\n",
    "                if non_zero_formats > 1:\n",
    "                    format_inconsistencies['phone_formatting'] = phone_patterns\n",
    "            \n",
    "            # Date formatting inconsistencies\n",
    "            if any(keyword in col.lower() for keyword in ['date', 'time', 'created', 'modified', 'birth', 'hire']):\n",
    "                date_patterns = {\n",
    "                    'mm_dd_yyyy': values.str.match(r'^\\d{1,2}/\\d{1,2}/\\d{4}$', na=False).sum(),\n",
    "                    'yyyy_mm_dd': values.str.match(r'^\\d{4}-\\d{1,2}-\\d{1,2}$', na=False).sum(),\n",
    "                    'dd_mm_yyyy': values.str.match(r'^\\d{1,2}-\\d{1,2}-\\d{4}$', na=False).sum(),\n",
    "                    'text_format': values.str.match(r'^[A-Za-z]{3}\\s\\d{1,2},?\\s\\d{4}$', na=False).sum()\n",
    "                }\n",
    "                non_zero_formats = sum(1 for count in date_patterns.values() if count > 0)\n",
    "                if non_zero_formats > 1:\n",
    "                    format_inconsistencies['date_formatting'] = date_patterns\n",
    "            \n",
    "            if format_inconsistencies:\n",
    "                col_errors['format_inconsistencies'] = format_inconsistencies\n",
    "                total_format_errors += len(format_inconsistencies)\n",
    "            \n",
    "            # 3. Check for unusual characters or encoding issues\n",
    "            unusual_chars = []\n",
    "            control_chars = []\n",
    "            for val in values.unique()[:100]:  # Check first 100 unique values\n",
    "                # Non-ASCII characters\n",
    "                if any(ord(char) > 127 for char in str(val)):\n",
    "                    unusual_chars.append(val)\n",
    "                # Control characters (except common whitespace)\n",
    "                if any(ord(char) < 32 and char not in ['\\t', '\\n', '\\r'] for char in str(val)):\n",
    "                    control_chars.append(val)\n",
    "            \n",
    "            if unusual_chars:\n",
    "                col_errors['non_ascii_characters'] = unusual_chars[:10]\n",
    "                total_format_errors += 1\n",
    "            \n",
    "            if control_chars:\n",
    "                col_errors['control_characters'] = control_chars[:10]\n",
    "                total_format_errors += 1\n",
    "            \n",
    "            # 4. Check for inconsistent text casing in what should be standardized fields\n",
    "            if any(keyword in col.lower() for keyword in ['state', 'country', 'status', 'type', 'category']):\n",
    "                unique_values = values.unique()\n",
    "                casing_issues = {\n",
    "                    'all_upper': sum(1 for val in unique_values if val.isupper()),\n",
    "                    'all_lower': sum(1 for val in unique_values if val.islower()),\n",
    "                    'title_case': sum(1 for val in unique_values if val.istitle()),\n",
    "                    'mixed_case': sum(1 for val in unique_values if not val.isupper() and not val.islower() and not val.istitle())\n",
    "                }\n",
    "                non_zero_cases = sum(1 for count in casing_issues.values() if count > 0)\n",
    "                if non_zero_cases > 1:\n",
    "                    col_errors['inconsistent_casing'] = casing_issues\n",
    "                    total_format_errors += 1\n",
    "            \n",
    "            # 5. Data Type Mismatch Detection - Enhanced\n",
    "            data_type_mismatches = {}\n",
    "            \n",
    "            # Check for numeric data stored as strings\n",
    "            potential_numeric_indicators = ['rate', 'salary', 'wage', 'amount', 'price', 'cost', 'value', 'number', 'count', 'total', 'sum', 'age', 'distance', 'hour']\n",
    "            is_likely_numeric = any(indicator in col.lower() for indicator in potential_numeric_indicators)\n",
    "            \n",
    "            if is_likely_numeric and len(values) > 0:\n",
    "                mismatch_details = {}\n",
    "                \n",
    "                # Check what's preventing numeric conversion\n",
    "                # Remove common formatting and see if it becomes numeric\n",
    "                cleaned_values = (values\n",
    "                                .str.replace(r'[\\$Â£â‚¬Â¥,\\s%]', '', regex=True)  # Remove currency, commas, spaces, %\n",
    "                                .str.replace(r'^[\\+\\-]?', '', regex=True))    # Remove leading +/-\n",
    "                \n",
    "                # Test if cleaned values are numeric\n",
    "                try:\n",
    "                    numeric_test = pd.to_numeric(cleaned_values, errors='coerce')\n",
    "                    convertible_count = numeric_test.notna().sum()\n",
    "                    convertible_percentage = (convertible_count / len(values)) * 100\n",
    "                    \n",
    "                    # If high percentage is convertible, it's likely a mismatch\n",
    "                    if convertible_percentage >= 80:  # 80% or more could be numeric\n",
    "                        mismatch_details['expected_type'] = 'numeric'\n",
    "                        mismatch_details['current_type'] = 'object'\n",
    "                        mismatch_details['convertible_percentage'] = convertible_percentage\n",
    "                        mismatch_details['sample_values'] = values.head(5).tolist()\n",
    "                        \n",
    "                        # Identify specific formatting issues\n",
    "                        formatting_issues = []\n",
    "                        if values.str.contains(r'[\\$Â£â‚¬Â¥]', na=False).any():\n",
    "                            formatting_issues.append('currency_symbols')\n",
    "                        if values.str.contains(',', na=False).any():\n",
    "                            formatting_issues.append('thousands_separators')\n",
    "                        if values.str.contains('%', na=False).any():\n",
    "                            formatting_issues.append('percentage_signs')\n",
    "                        if values.str.contains(r'\\s', na=False).any():\n",
    "                            formatting_issues.append('embedded_spaces')\n",
    "                        \n",
    "                        mismatch_details['formatting_issues'] = formatting_issues\n",
    "                        mismatch_details['recommended_fix'] = f\"Convert to numeric after removing formatting characters\"\n",
    "                        \n",
    "                        data_type_mismatches[col] = mismatch_details\n",
    "                        print(f\"      ðŸš¨ Data Type Mismatch detected in '{col}': {convertible_percentage:.1f}% convertible to numeric\")\n",
    "                        print(f\"         Issues: {', '.join(formatting_issues)}\")\n",
    "                        print(f\"         Sample values: {mismatch_details['sample_values']}\")\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # Check for date columns stored as strings\n",
    "            date_indicators = ['date', 'time', 'created', 'modified', 'birth', 'hire', 'start', 'end', 'updated']\n",
    "            is_likely_date = any(indicator in col.lower() for indicator in date_indicators)\n",
    "            \n",
    "            if is_likely_date and len(values) > 0:\n",
    "                # Test date conversion\n",
    "                try:\n",
    "                    date_test = pd.to_datetime(values.head(100), errors='coerce')  # Test first 100\n",
    "                    convertible_count = date_test.notna().sum()\n",
    "                    convertible_percentage = (convertible_count / min(100, len(values))) * 100\n",
    "                    \n",
    "                    if convertible_percentage >= 70:  # 70% or more could be dates\n",
    "                        if col not in data_type_mismatches:\n",
    "                            data_type_mismatches[col] = {}\n",
    "                        data_type_mismatches[col].update({\n",
    "                            'expected_type': 'datetime',\n",
    "                            'current_type': 'object',\n",
    "                            'convertible_percentage': convertible_percentage,\n",
    "                            'sample_values': values.head(3).tolist(),\n",
    "                            'recommended_fix': \"Convert to datetime using pd.to_datetime()\"\n",
    "                        })\n",
    "                        print(f\"  Date Type Mismatch detected in '{col}': {convertible_percentage:.1f}% convertible to datetime\")\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            if data_type_mismatches:\n",
    "                col_errors['data_type_mismatches'] = data_type_mismatches\n",
    "                total_format_errors += len(data_type_mismatches)\n",
    "        \n",
    "        # 6. Check numeric columns stored as strings with formatting characters\n",
    "        elif col in numeric_columns and df[col].dtype == 'object':\n",
    "            try:\n",
    "                # Try to identify what's preventing numeric conversion\n",
    "                non_numeric_reasons = {}\n",
    "                string_values = df[col].dropna().astype(str)\n",
    "                \n",
    "                # Check for currency symbols\n",
    "                has_currency = string_values.str.contains(r'[\\$Â£â‚¬Â¥]', na=False).sum()\n",
    "                if has_currency > 0:\n",
    "                    non_numeric_reasons['currency_symbols'] = has_currency\n",
    "                \n",
    "                # Check for commas\n",
    "                has_commas = string_values.str.contains(',', na=False).sum()\n",
    "                if has_commas > 0:\n",
    "                    non_numeric_reasons['thousands_separators'] = has_commas\n",
    "                \n",
    "                # Check for percentage signs\n",
    "                has_percent = string_values.str.contains('%', na=False).sum()\n",
    "                if has_percent > 0:\n",
    "                    non_numeric_reasons['percentage_signs'] = has_percent\n",
    "                \n",
    "                # Check for spaces\n",
    "                has_spaces = string_values.str.contains(' ', na=False).sum()\n",
    "                if has_spaces > 0:\n",
    "                    non_numeric_reasons['embedded_spaces'] = has_spaces\n",
    "                \n",
    "                if non_numeric_reasons:\n",
    "                    col_errors['numeric_formatting_issues'] = non_numeric_reasons\n",
    "                    total_format_errors += 1\n",
    "                    print(f\"   Numeric column '{col}' has formatting issues: {', '.join(non_numeric_reasons.keys())}\")\n",
    "                \n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        if col_errors:\n",
    "            formatting_errors[col] = col_errors\n",
    "    \n",
    "    report['formatting_errors'] = formatting_errors\n",
    "    print(f\"  Formatting error types found: {total_format_errors}\")\n",
    "    \n",
    "    if formatting_errors:\n",
    "        print(\" Columns with formatting errors:\")\n",
    "        for col, errors in list(formatting_errors.items())[:5]:\n",
    "            error_types = list(errors.keys())\n",
    "            print(f\"      â€¢ {col}: {', '.join(error_types)}\")\n",
    "    \n",
    "    # 5. OUTLIERS\n",
    "    print(\"\\n 5. CHECKING FOR OUTLIERS...\")\n",
    "    \n",
    "    outlier_report = {}\n",
    "    total_outliers = 0\n",
    "    \n",
    "    for col in numeric_columns:\n",
    "        if col in df.columns and pd.api.types.is_numeric_dtype(df[col]):\n",
    "            col_data = df[col].dropna()\n",
    "            \n",
    "            if len(col_data) == 0:\n",
    "                continue\n",
    "                \n",
    "            outliers = {}\n",
    "            \n",
    "            if outlier_method == 'iqr':\n",
    "                Q1 = col_data.quantile(0.25)\n",
    "                Q3 = col_data.quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - outlier_threshold * IQR\n",
    "                upper_bound = Q3 + outlier_threshold * IQR\n",
    "                \n",
    "                outlier_mask = (col_data < lower_bound) | (col_data > upper_bound)\n",
    "                outlier_values = col_data[outlier_mask]\n",
    "                \n",
    "                outliers['method'] = 'IQR'\n",
    "                outliers['bounds'] = {'lower': float(lower_bound), 'upper': float(upper_bound)}\n",
    "                \n",
    "            elif outlier_method == 'zscore':\n",
    "                z_scores = np.abs((col_data - col_data.mean()) / col_data.std())\n",
    "                outlier_mask = z_scores > outlier_threshold\n",
    "                outlier_values = col_data[outlier_mask]\n",
    "                \n",
    "                outliers['method'] = 'Z-Score'\n",
    "                outliers['threshold'] = outlier_threshold\n",
    "                \n",
    "            elif outlier_method == 'modified_zscore':\n",
    "                median = col_data.median()\n",
    "                mad = np.median(np.abs(col_data - median))\n",
    "                if mad != 0:\n",
    "                    modified_z_scores = 0.6745 * (col_data - median) / mad\n",
    "                    outlier_mask = np.abs(modified_z_scores) > outlier_threshold\n",
    "                    outlier_values = col_data[outlier_mask]\n",
    "                else:\n",
    "                    outlier_values = pd.Series([], dtype=col_data.dtype)\n",
    "                \n",
    "                outliers['method'] = 'Modified Z-Score'\n",
    "                outliers['threshold'] = outlier_threshold\n",
    "            \n",
    "            if len(outlier_values) > 0:\n",
    "                outliers.update({\n",
    "                    'count': len(outlier_values),\n",
    "                    'percentage': (len(outlier_values) / len(col_data)) * 100,\n",
    "                    'values': outlier_values.tolist()[:50],  # Limit to first 50\n",
    "                    'indices': outlier_values.index.tolist()[:50],\n",
    "                    'statistics': {\n",
    "                        'min_outlier': float(outlier_values.min()),\n",
    "                        'max_outlier': float(outlier_values.max()),\n",
    "                        'mean_outlier': float(outlier_values.mean())\n",
    "                    }\n",
    "                })\n",
    "                \n",
    "                outlier_report[col] = outliers\n",
    "                total_outliers += len(outlier_values)\n",
    "    \n",
    "    report['outliers'] = outlier_report\n",
    "    print(f\"   âœ“ Total outliers found: {total_outliers:,}\")\n",
    "    \n",
    "    if outlier_report and show_plots:\n",
    "        # Visualization: Outliers\n",
    "        n_outlier_cols = len(outlier_report)\n",
    "        if n_outlier_cols > 0:\n",
    "            fig, axes = plt.subplots(min(2, n_outlier_cols), 2, figsize=(15, min(8, n_outlier_cols * 4)))\n",
    "            if n_outlier_cols == 1:\n",
    "                axes = axes.reshape(1, -1)\n",
    "            \n",
    "            for i, (col, stats) in enumerate(list(outlier_report.items())[:2]):\n",
    "                row = i\n",
    "                \n",
    "                # Box plot\n",
    "                ax1 = axes[row, 0] if n_outlier_cols > 1 else axes[0]\n",
    "                df[col].plot.box(ax=ax1)\n",
    "                ax1.set_title(f'{col} - Box Plot')\n",
    "                ax1.set_ylabel('Values')\n",
    "                \n",
    "                # Histogram\n",
    "                ax2 = axes[row, 1] if n_outlier_cols > 1 else axes[1]\n",
    "                df[col].hist(bins=50, alpha=0.7, ax=ax2)\n",
    "                ax2.axvline(stats['bounds']['lower'] if 'bounds' in stats else df[col].mean() - 2*df[col].std(), \n",
    "                           color='red', linestyle='--', alpha=0.7, label='Lower bound')\n",
    "                ax2.axvline(stats['bounds']['upper'] if 'bounds' in stats else df[col].mean() + 2*df[col].std(), \n",
    "                           color='red', linestyle='--', alpha=0.7, label='Upper bound')\n",
    "                ax2.set_title(f'{col} - Distribution')\n",
    "                ax2.set_xlabel('Values')\n",
    "                ax2.set_ylabel('Frequency')\n",
    "                ax2.legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "    if outlier_report:\n",
    "        print(\"   ðŸ“‹ Columns with outliers:\")\n",
    "        for col, stats in list(outlier_report.items())[:5]:\n",
    "            print(f\"      â€¢ {col}: {stats['count']:,} outliers ({stats['percentage']:.2f}%)\")\n",
    "    \n",
    "    # 6. SUMMARY\n",
    "    print(\"\\nðŸ“Š GENERATING SUMMARY...\")\n",
    "    \n",
    "    total_issues = 0\n",
    "    issue_categories = []\n",
    "    \n",
    "    if report['duplicates']['full_row_duplicates']['count'] > 0:\n",
    "        total_issues += report['duplicates']['full_row_duplicates']['count']\n",
    "        issue_categories.append('duplicates')\n",
    "    \n",
    "    missing_issues = sum([stats['count'] for stats in report['missing_values'].values()])\n",
    "    if missing_issues > 0:\n",
    "        total_issues += missing_issues\n",
    "        issue_categories.append('missing_values')\n",
    "    \n",
    "    if report['inconsistent_entries']:\n",
    "        total_issues += total_inconsistencies\n",
    "        issue_categories.append('inconsistent_entries')\n",
    "    \n",
    "    if report['formatting_errors']:\n",
    "        total_issues += total_format_errors\n",
    "        issue_categories.append('formatting_errors')\n",
    "    \n",
    "    if total_outliers > 0:\n",
    "        total_issues += total_outliers\n",
    "        issue_categories.append('outliers')\n",
    "    \n",
    "    # Calculate data quality score\n",
    "    max_possible_issues = len(df) * len(df.columns)\n",
    "    quality_score = max(0, 100 - (total_issues / max_possible_issues) * 100)\n",
    "    \n",
    "    report['summary'] = {\n",
    "        'total_issues_found': total_issues,\n",
    "        'issue_categories': issue_categories,\n",
    "        'data_quality_score': quality_score,\n",
    "        'recommendations': []\n",
    "    }\n",
    "    \n",
    "    # Add recommendations\n",
    "    recommendations = []\n",
    "    if report['duplicates']['full_row_duplicates']['count'] > 0:\n",
    "        recommendations.append(\"Remove duplicate rows to improve data integrity\")\n",
    "    if missing_issues > 0:\n",
    "        recommendations.append(\"Handle missing values through imputation or removal\")\n",
    "    if report['inconsistent_entries']:\n",
    "        recommendations.append(\"Standardize categorical values and fix case/whitespace issues\")\n",
    "    if report['formatting_errors']:\n",
    "        recommendations.append(\"Clean formatting errors and ensure consistent data types\")\n",
    "    if total_outliers > 0:\n",
    "        recommendations.append(\"Investigate outliers - validate if they're errors or legitimate extreme values\")\n",
    "    \n",
    "    report['summary']['recommendations'] = recommendations\n",
    "    \n",
    "    # Final Summary Display\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DATA QUALITY INSPECTION SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Data Quality Score: {quality_score:.1f}/100\")\n",
    "    print(f\"Total Issues Found: {total_issues:,}\")\n",
    "    print(f\"Dataset Size: {len(df):,} rows Ã— {len(df.columns)} columns\")\n",
    "    print(f\"Memory Usage: {report['dataset_overview']['memory_usage'] / 1024**2:.2f} MB\")\n",
    "    \n",
    "    if issue_categories:\n",
    "        print(f\"\\nIssue Categories Found:\")\n",
    "        for category in issue_categories:\n",
    "            print(f\"   â€¢ {category.replace('_', ' ').title()}\")\n",
    "    \n",
    "    if recommendations:\n",
    "        print(f\"\\nRecommendations:\")\n",
    "        for rec in recommendations:\n",
    "            print(f\"   {rec}\")\n",
    "    \n",
    "    print(\"\\nData quality inspection completed!\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return report\n",
    "\n",
    "def get_column_summary_table(df: pd.DataFrame, report: Dict[str, Any]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get a clean summary table of all columns with the requested metrics\n",
    "    \n",
    "    Returns a pandas DataFrame that can be easily displayed or exported\n",
    "    \"\"\"\n",
    "    column_summary = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_data = df[col]\n",
    "        \n",
    "        # Data Type\n",
    "        data_type = str(col_data.dtype)\n",
    "        \n",
    "        # Missing Data\n",
    "        missing_count = col_data.isnull().sum()\n",
    "        missing_percentage = (missing_count / len(col_data)) * 100\n",
    "        \n",
    "        # Whitespace Issues\n",
    "        whitespace_count = 0\n",
    "        if col_data.dtype == 'object':\n",
    "            non_null_values = col_data.dropna().astype(str)\n",
    "            whitespace_mask = non_null_values != non_null_values.str.strip()\n",
    "            whitespace_count = whitespace_mask.sum()\n",
    "        \n",
    "        # Case Inconsistencies\n",
    "        case_inconsistencies = 0\n",
    "        if col in report['inconsistent_entries']:\n",
    "            if 'case_variations' in report['inconsistent_entries'][col]:\n",
    "                case_variations = report['inconsistent_entries'][col]['case_variations']\n",
    "                case_inconsistencies = len(case_variations)\n",
    "        \n",
    "        # Outlier Count\n",
    "        outlier_count = 0\n",
    "        if col in report['outliers']:\n",
    "            outlier_count = report['outliers'][col]['count']\n",
    "        \n",
    "        # Absurd Values\n",
    "        absurd_info = detect_absurd_values(df, col)\n",
    "        absurd_count = absurd_info['count']\n",
    "        \n",
    "        column_summary.append({\n",
    "            'Column': col,\n",
    "            'Data_Type': data_type,\n",
    "            'Missing_Data_Count': missing_count,\n",
    "            'Missing_Data_Percentage': round(missing_percentage, 2),\n",
    "            'Absurd_Values': absurd_count,\n",
    "            'Outlier_Count': outlier_count,\n",
    "            'Whitespace_Issues': whitespace_count,\n",
    "            'Case_Inconsistencies': case_inconsistencies\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(column_summary)\n",
    "    return summary_df\n",
    "\n",
    "def detect_absurd_values(df: pd.DataFrame, column: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Detect absurd values based on business logic and common sense rules\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe\n",
    "    column : str\n",
    "        Column name to check\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Dict[str, Any] : Report of absurd values found\n",
    "    \"\"\"\n",
    "    \n",
    "    absurd_report = {\n",
    "        'count': 0,\n",
    "        'percentage': 0.0,\n",
    "        'values': [],\n",
    "        'indices': [],\n",
    "        'rules_violated': []\n",
    "    }\n",
    "    \n",
    "    if column not in df.columns:\n",
    "        return absurd_report\n",
    "    \n",
    "    col_data = df[column].dropna()\n",
    "    if len(col_data) == 0:\n",
    "        return absurd_report\n",
    "    \n",
    "    # Only check numeric columns for absurd values\n",
    "    if not pd.api.types.is_numeric_dtype(col_data):\n",
    "        return absurd_report\n",
    "    \n",
    "    absurd_mask = pd.Series([False] * len(col_data), index=col_data.index)\n",
    "    rules_violated = []\n",
    "    \n",
    "    # Define business logic rules based on column names and typical HR data\n",
    "    column_lower = column.lower()\n",
    "    \n",
    "    # Age-related rules\n",
    "    if 'age' in column_lower:\n",
    "        age_absurd = (col_data < 16) | (col_data > 100)\n",
    "        absurd_mask |= age_absurd\n",
    "        if age_absurd.any():\n",
    "            rules_violated.append(f\"Age values outside reasonable range (16-100): found {age_absurd.sum()} values\")\n",
    "    \n",
    "    # Tenure-related rules\n",
    "    elif 'tenure' in column_lower or 'experience' in column_lower:\n",
    "        tenure_absurd = (col_data < 0) | (col_data > 50)\n",
    "        absurd_mask |= tenure_absurd\n",
    "        if tenure_absurd.any():\n",
    "            rules_violated.append(f\"Tenure/Experience values outside reasonable range (0-50 years): found {tenure_absurd.sum()} values\")\n",
    "    \n",
    "    # Distance-related rules (like DrivingCommuterDistance)\n",
    "    elif 'distance' in column_lower or 'commut' in column_lower:\n",
    "        distance_absurd = (col_data < 0) | (col_data > 500)  # Negative or > 500 miles is absurd\n",
    "        absurd_mask |= distance_absurd\n",
    "        if distance_absurd.any():\n",
    "            rules_violated.append(f\"Distance values outside reasonable range (0-500): found {distance_absurd.sum()} values\")\n",
    "    \n",
    "    # Hours-related rules\n",
    "    elif 'hour' in column_lower and 'weekly' in column_lower:\n",
    "        hours_absurd = (col_data < 0) | (col_data > 80)  # Negative or >80 hours/week is absurd\n",
    "        absurd_mask |= hours_absurd\n",
    "        if hours_absurd.any():\n",
    "            rules_violated.append(f\"Weekly hours outside reasonable range (0-80): found {hours_absurd.sum()} values\")\n",
    "    \n",
    "    elif 'hour' in column_lower and ('annual' in column_lower or 'professional' in column_lower or 'dev' in column_lower):\n",
    "        hours_absurd = (col_data < 0) | (col_data > 2000)  # Annual professional dev hours\n",
    "        absurd_mask |= hours_absurd\n",
    "        if hours_absurd.any():\n",
    "            rules_violated.append(f\"Annual hours outside reasonable range (0-2000): found {hours_absurd.sum()} values\")\n",
    "    \n",
    "    # Salary-related rules\n",
    "    elif 'annualsalary' in column_lower: # or 'rate' in column_lower:\n",
    "        # Check for negative salaries first (always absurd)\n",
    "        negative_salary = col_data < 0\n",
    "        if negative_salary.any():\n",
    "            absurd_mask |= negative_salary\n",
    "            rules_violated.append(f\"Negative salary values found (impossible): {negative_salary.sum()} values\")\n",
    "        \n",
    "        # Then check reasonable ranges for positive values\n",
    "        positive_data = col_data[col_data >= 0]\n",
    "        if len(positive_data) > 0:\n",
    "            if len(positive_data) > 0:\n",
    "        # Only salaries > $1M are truly absurd (CEO pay can be high, but > $1M annual is very rare)\n",
    "                extremely_high = positive_data > 1000000\n",
    "                if extremely_high.any():\n",
    "                    absurd_mask |= extremely_high\n",
    "                    rules_violated.append(f\"Extremely high salary values (> $1M): found {extremely_high.sum()} values\")\n",
    "            else:  # Likely hourly rate\n",
    "                rate_absurd = (positive_data < 7.25) | (positive_data > 200)\n",
    "                absurd_mask |= rate_absurd\n",
    "                if rate_absurd.any():\n",
    "                    rules_violated.append(f\"Hourly rate outside reasonable range ($7.25-$200): found {rate_absurd.sum()} values\")\n",
    "    \n",
    "    # Employee number rules\n",
    "    elif 'employee' in column_lower and 'number' in column_lower:\n",
    "        emp_absurd = col_data <= 0\n",
    "        absurd_mask |= emp_absurd\n",
    "        if emp_absurd.any():\n",
    "            rules_violated.append(f\"Employee numbers <= 0: found {emp_absurd.sum()} values\")\n",
    "    \n",
    "    # Number of companies worked\n",
    "    elif 'compan' in column_lower and ('previous' in column_lower or 'worked' in column_lower):\n",
    "        companies_absurd = (col_data < 0) | (col_data > 20)  # Negative or more than 20 previous companies\n",
    "        absurd_mask |= companies_absurd\n",
    "        if companies_absurd.any():\n",
    "            rules_violated.append(f\"Number of previous companies outside reasonable range (0-20): found {companies_absurd.sum()} values\")\n",
    "    \n",
    "    # General negative value check for columns that should always be positive\n",
    "    positive_keywords = ['count', 'number', 'total', 'amount']\n",
    "    if any(keyword in column_lower for keyword in positive_keywords):\n",
    "        negative_absurd = col_data < 0\n",
    "        absurd_mask |= negative_absurd\n",
    "        if negative_absurd.any():\n",
    "            rules_violated.append(f\"Negative values in column that should be positive: found {negative_absurd.sum()} values\")\n",
    "    \n",
    "    # Extract results\n",
    "    if absurd_mask.any():\n",
    "        absurd_values = col_data[absurd_mask]\n",
    "        absurd_report = {\n",
    "            'count': len(absurd_values),\n",
    "            'percentage': (len(absurd_values) / len(col_data)) * 100,\n",
    "            'values': absurd_values.tolist()[:20],  # Limit to first 20\n",
    "            'indices': absurd_values.index.tolist()[:20],\n",
    "            'rules_violated': rules_violated\n",
    "        }\n",
    "    \n",
    "    return absurd_report\n",
    "\n",
    "def print_detailed_column_report(df: pd.DataFrame, report: Dict[str, Any]) -> None:\n",
    "    \"\"\"\n",
    "    Print a detailed, formatted column-by-column report\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*90)\n",
    "    print(\"ðŸ“‹ DETAILED COLUMN-BY-COLUMN DATA QUALITY REPORT\")\n",
    "    print(\"=\"*90)\n",
    "    \n",
    "    for col in df.columns:\n",
    "        print(f\"\\nðŸ” COLUMN: '{col}'\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        col_data = df[col]\n",
    "        \n",
    "        # 1. Data Type\n",
    "        print(f\"ðŸ“Š Data Type: {col_data.dtype}\")\n",
    "        \n",
    "        # 2. Missing Data\n",
    "        missing_count = col_data.isnull().sum()\n",
    "        missing_percentage = (missing_count / len(col_data)) * 100\n",
    "        print(f\"â“ Missing Data: {missing_count:,} ({missing_percentage:.2f}%)\")\n",
    "        \n",
    "        # Check for alternative missing representations\n",
    "        if col in report['missing_values'] and 'potential_missing_representations' in report['missing_values'][col]:\n",
    "            alt_missing = report['missing_values'][col]['potential_missing_representations']\n",
    "            print(f\"   â€¢ Alternative missing representations: {alt_missing['count']} values\")\n",
    "            print(f\"     â†’ {list(alt_missing['values'].keys())}\")\n",
    "        \n",
    "        # 3. Absurd Values\n",
    "        absurd_info = detect_absurd_values(df, col)\n",
    "        absurd_count = absurd_info['count']\n",
    "        print(f\"ðŸš¨ Absurd Values: {absurd_count} ({absurd_info['percentage']:.2f}%)\")\n",
    "        \n",
    "        if absurd_count > 0:\n",
    "            print(f\"   â€¢ Rules violated:\")\n",
    "            for rule in absurd_info['rules_violated']:\n",
    "                print(f\"     â†’ {rule}\")\n",
    "            if absurd_info['values']:\n",
    "                print(f\"   â€¢ Sample absurd values: {absurd_info['values'][:5]}\")\n",
    "        \n",
    "        # 4. Outlier Count\n",
    "        outlier_count = 0\n",
    "        if col in report['outliers']:\n",
    "            outlier_info = report['outliers'][col]\n",
    "            outlier_count = outlier_info['count']\n",
    "            print(f\"ðŸ“ˆ Outlier Count: {outlier_count} ({outlier_info['percentage']:.2f}%)\")\n",
    "            print(f\"   â€¢ Method: {outlier_info['method']}\")\n",
    "            if 'bounds' in outlier_info:\n",
    "                print(f\"   â€¢ Bounds: [{outlier_info['bounds']['lower']:.2f}, {outlier_info['bounds']['upper']:.2f}]\")\n",
    "            if outlier_info['values']:\n",
    "                sample_outliers = outlier_info['values'][:5]\n",
    "                print(f\"   â€¢ Sample outliers: {sample_outliers}\")\n",
    "        else:\n",
    "            print(f\"ðŸ“ˆ Outlier Count: {outlier_count}\")\n",
    "        \n",
    "        # 5. Whitespace Issues\n",
    "        whitespace_count = 0\n",
    "        whitespace_examples = []\n",
    "        if col_data.dtype == 'object':\n",
    "            non_null_values = col_data.dropna().astype(str)\n",
    "            whitespace_mask = non_null_values != non_null_values.str.strip()\n",
    "            whitespace_count = whitespace_mask.sum()\n",
    "            if whitespace_count > 0:\n",
    "                whitespace_examples = non_null_values[whitespace_mask].unique()[:3].tolist()\n",
    "        \n",
    "        print(f\"ðŸ”¤ Whitespace Issues: {whitespace_count}\")\n",
    "        if whitespace_examples:\n",
    "            print(f\"   â€¢ Examples: {whitespace_examples}\")\n",
    "        \n",
    "        # 6. Case Inconsistencies\n",
    "        case_inconsistencies = 0\n",
    "        if col in report['inconsistent_entries']:\n",
    "            if 'case_variations' in report['inconsistent_entries'][col]:\n",
    "                case_variations = report['inconsistent_entries'][col]['case_variations']\n",
    "                case_inconsistencies = len(case_variations)\n",
    "                print(f\"ðŸ”  Case Inconsistencies: {case_inconsistencies} groups\")\n",
    "                \n",
    "                # Show examples\n",
    "                for group_name, variations in list(case_variations.items())[:2]:\n",
    "                    print(f\"   â€¢ '{group_name}' group: {variations}\")\n",
    "            else:\n",
    "                print(f\"ðŸ”  Case Inconsistencies: {case_inconsistencies}\")\n",
    "        else:\n",
    "            print(f\"ðŸ”  Case Inconsistencies: {case_inconsistencies}\")\n",
    "        \n",
    "        # Additional standardization issues\n",
    "        if col in report['inconsistent_entries']:\n",
    "            if 'standardization_issues' in report['inconsistent_entries'][col]:\n",
    "                std_issues = report['inconsistent_entries'][col]['standardization_issues']\n",
    "                print(f\"ðŸŽ¯ Standardization Groups: {len(std_issues)}\")\n",
    "                for group_name, variants in list(std_issues.items())[:2]:\n",
    "                    variant_names = [v['value'] for v in variants]\n",
    "                    print(f\"   â€¢ '{group_name}': {variant_names}\")\n",
    "        \n",
    "        # Summary for this column\n",
    "        total_issues = missing_count + absurd_count + outlier_count + whitespace_count + case_inconsistencies\n",
    "        issue_density = (total_issues / len(col_data)) * 100\n",
    "        \n",
    "        if issue_density == 0:\n",
    "            print(f\"âœ… Column Quality: EXCELLENT (no issues found)\")\n",
    "        elif issue_density < 1:\n",
    "            print(f\"ðŸŸ¢ Column Quality: GOOD ({issue_density:.2f}% issues)\")\n",
    "        elif issue_density < 5:\n",
    "            print(f\"ðŸŸ¡ Column Quality: FAIR ({issue_density:.2f}% issues)\")\n",
    "        else:\n",
    "            print(f\"ðŸ”´ Column Quality: POOR ({issue_density:.2f}% issues)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*90)\n",
    "\n",
    "def generate_standardization_recommendations(report: Dict[str, Any], df: pd.DataFrame) -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Generate specific recommendations for standardizing categorical values\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    report : Dict[str, Any]\n",
    "        The quality report from inspect_data_quality()\n",
    "    df : pd.DataFrame\n",
    "        The original dataframe\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Dict[str, Dict] : Standardization recommendations by column\n",
    "    \"\"\"\n",
    "    \n",
    "    recommendations = {}\n",
    "    \n",
    "    for col, issues in report['inconsistent_entries'].items():\n",
    "        if 'standardization_issues' in issues:\n",
    "            col_recommendations = {}\n",
    "            standardization_groups = issues['standardization_issues']\n",
    "            \n",
    "            for group_name, variants in standardization_groups.items():\n",
    "                # Determine the best standard format\n",
    "                variant_counts = [(item['value'], item['count']) for item in variants]\n",
    "                \n",
    "                # Strategy 1: Use the most common variant\n",
    "                most_common = max(variant_counts, key=lambda x: x[1])\n",
    "                \n",
    "                # Strategy 2: Use the cleanest format (prefer spaces over underscores, proper case)\n",
    "                def format_score(value):\n",
    "                    score = 0\n",
    "                    # Prefer spaces over underscores\n",
    "                    if ' ' in value and '_' not in value:\n",
    "                        score += 3\n",
    "                    # Prefer proper case\n",
    "                    if value.replace('_', ' ').title() == value.replace('_', ' '):\n",
    "                        score += 2\n",
    "                    # Prefer shorter, cleaner formats\n",
    "                    score -= len(value) * 0.1\n",
    "                    return score\n",
    "                \n",
    "                cleanest = max(variant_counts, key=lambda x: format_score(x[0]))\n",
    "                \n",
    "                # Create mapping recommendations\n",
    "                mapping = {}\n",
    "                total_affected = sum([count for _, count in variant_counts])\n",
    "                \n",
    "                # Use the most common as standard unless cleanest format is significantly better\n",
    "                if cleanest[1] >= most_common[1] * 0.5:  # Cleanest has at least 50% of most common's frequency\n",
    "                    standard_value = cleanest[0]\n",
    "                else:\n",
    "                    standard_value = most_common[0]\n",
    "                \n",
    "                for value, count in variant_counts:\n",
    "                    if value != standard_value:\n",
    "                        mapping[value] = standard_value\n",
    "                \n",
    "                if mapping:\n",
    "                    col_recommendations[group_name] = {\n",
    "                        'standard_value': standard_value,\n",
    "                        'mappings': mapping,\n",
    "                        'total_affected_rows': total_affected,\n",
    "                        'variants': variant_counts\n",
    "                    }\n",
    "            \n",
    "            if col_recommendations:\n",
    "                recommendations[col] = col_recommendations\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "def print_standardization_recommendations(recommendations: Dict[str, Dict], df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Print formatted standardization recommendations\n",
    "    \"\"\"\n",
    "    if not recommendations:\n",
    "        print(\"âœ… No standardization issues found!\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ðŸŽ¯ STANDARDIZATION RECOMMENDATIONS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for col, col_recommendations in recommendations.items():\n",
    "        print(f\"\\nðŸ“Š COLUMN: '{col}'\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        total_rows_affected = sum([rec['total_affected_rows'] for rec in col_recommendations.values()])\n",
    "        print(f\"Total rows that need standardization: {total_rows_affected:,} ({total_rows_affected/len(df)*100:.1f}%)\")\n",
    "        \n",
    "        for group_name, rec in col_recommendations.items():\n",
    "            print(f\"\\nðŸ”„ Group: '{group_name}' â†’ Standardize to: '{rec['standard_value']}'\")\n",
    "            print(f\"   Affected rows: {rec['total_affected_rows']:,}\")\n",
    "            print(f\"   Variants to change:\")\n",
    "            \n",
    "            for old_value, new_value in rec['mappings'].items():\n",
    "                old_count = next(count for val, count in rec['variants'] if val == old_value)\n",
    "                print(f\"      â€¢ '{old_value}' ({old_count:,} rows) â†’ '{new_value}'\")\n",
    "        \n",
    "        # Generate pandas code\n",
    "        print(f\"\\nðŸ’» Pandas code to fix '{col}':\")\n",
    "        print(\"```python\")\n",
    "        mapping_dict = {}\n",
    "        for rec in col_recommendations.values():\n",
    "            mapping_dict.update(rec['mappings'])\n",
    "        \n",
    "        print(f\"# Standardize {col}\")\n",
    "        print(f\"mapping_{col.lower().replace(' ', '_')} = {mapping_dict}\")\n",
    "        print(f\"df['{col}'] = df['{col}'].replace(mapping_{col.lower().replace(' ', '_')})\")\n",
    "        print(\"```\")\n",
    "\n",
    "def detect_delimiter_inconsistencies(df: pd.DataFrame, columns: List[str] = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Detect inconsistent use of delimiters (underscores, spaces, hyphens) in categorical columns\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe\n",
    "        columns : List[str], optional\n",
    "        Specific columns to check. If None, checks all object columns\n",
    "    Returns:\n",
    "    --------\n",
    "    Dict[str, Any] : Report of delimiter inconsistencies\n",
    "    \"\"\"\n",
    "    \n",
    "    if columns is None:\n",
    "        columns = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    delimiter_report = {}\n",
    "    \n",
    "    for col in columns:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        values = df[col].dropna().astype(str).unique()\n",
    "        \n",
    "        # Check for mixed delimiter usage\n",
    "        has_underscores = any('_' in str(val) for val in values)\n",
    "        has_spaces = any(' ' in str(val) for val in values)\n",
    "        has_hyphens = any('-' in str(val) for val in values)\n",
    "        \n",
    "        delimiter_count = sum([has_underscores, has_spaces, has_hyphens])\n",
    "        \n",
    "        if delimiter_count > 1:  # Mixed delimiters found\n",
    "            delimiter_analysis = {\n",
    "                'mixed_delimiters': True,\n",
    "                'delimiters_found': [],\n",
    "                'examples': {}\n",
    "            }\n",
    "            \n",
    "            if has_underscores:\n",
    "                delimiter_analysis['delimiters_found'].append('underscore')\n",
    "                underscore_examples = [val for val in values if '_' in str(val)][:3]\n",
    "                delimiter_analysis['examples']['underscore'] = underscore_examples\n",
    "            \n",
    "            if has_spaces:\n",
    "                delimiter_analysis['delimiters_found'].append('space')\n",
    "                space_examples = [val for val in values if ' ' in str(val)][:3]\n",
    "                delimiter_analysis['examples']['space'] = space_examples\n",
    "            \n",
    "            if has_hyphens:\n",
    "                delimiter_analysis['delimiters_found'].append('hyphen')\n",
    "                hyphen_examples = [val for val in values if '-' in str(val)][:3]\n",
    "                delimiter_analysis['examples']['hyphen'] = hyphen_examples\n",
    "            \n",
    "            delimiter_report[col] = delimiter_analysis\n",
    "    \n",
    "    return delimiter_report\n",
    "\n",
    "def get_detailed_column_report(df: pd.DataFrame, column: str, report: Dict[str, Any]) -> None:\n",
    "    \"\"\"\n",
    "    Display detailed information about a specific column\n",
    "    \"\"\"\n",
    "    if column not in df.columns:\n",
    "        print(f\"Column '{column}' not found in dataset\")\n",
    "        return\n",
    "    \n",
    "    print(f\"DETAILED REPORT FOR COLUMN: '{column}'\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    col_data = df[column]\n",
    "    \n",
    "    # Basic info\n",
    "    print(f\"Data Type: {col_data.dtype}\")\n",
    "    print(f\"Non-null Count: {col_data.count():,} / {len(col_data):,}\")\n",
    "    print(f\"Missing Values: {col_data.isnull().sum():,} ({(col_data.isnull().sum()/len(col_data)*100):.2f}%)\")\n",
    "    print(f\"Unique Values: {col_data.nunique():,}\")\n",
    "    \n",
    "    # Type-specific analysis\n",
    "    if pd.api.types.is_numeric_dtype(col_data):\n",
    "        print(f\"\\nNUMERIC STATISTICS:\")\n",
    "        print(f\"Mean: {col_data.mean():.2f}\")\n",
    "        print(f\"Median: {col_data.median():.2f}\")\n",
    "        print(f\"Std Dev: {col_data.std():.2f}\")\n",
    "        print(f\"Min: {col_data.min()}\")\n",
    "        print(f\"Max: {col_data.max()}\")\n",
    "        \n",
    "        # Show outliers if any\n",
    "        if column in report['outliers']:\n",
    "            outlier_info = report['outliers'][column]\n",
    "            print(f\"\\nOUTLIERS ({outlier_info['method']}):\")\n",
    "            print(f\"Count: {outlier_info['count']:,} ({outlier_info['percentage']:.2f}%)\")\n",
    "            if 'bounds' in outlier_info:\n",
    "                print(f\"Bounds: [{outlier_info['bounds']['lower']:.2f}, {outlier_info['bounds']['upper']:.2f}]\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"\\nCATEGORICAL STATISTICS:\")\n",
    "        value_counts = col_data.value_counts().head(10)\n",
    "        print(\"Top 10 values:\")\n",
    "        for val, count in value_counts.items():\n",
    "            print(f\"   '{val}': {count:,} ({count/len(col_data)*100:.1f}%)\")\n",
    "        \n",
    "        # Show inconsistencies if any\n",
    "        if column in report['inconsistent_entries']:\n",
    "            inconsistencies = report['inconsistent_entries'][column]\n",
    "            print(f\"\\nINCONSISTENCIES FOUND:\")\n",
    "            for issue_type, details in inconsistencies.items():\n",
    "                print(f\"   {issue_type.replace('_', ' ').title()}: {len(details)} issues\")\n",
    "    \n",
    "    # Show formatting errors if any\n",
    "    if column in report['formatting_errors']:\n",
    "        format_errors = report['formatting_errors'][column]\n",
    "        print(f\"\\nFORMATTING ERRORS:\")\n",
    "        for error_type, details in format_errors.items():\n",
    "            print(f\"   {error_type.replace('_', ' ').title()}: {len(details)} issues\")\n",
    "\n",
    "# Example usage for Jupyter Notebook:\n",
    "def quick_quality_check(csv_file_path: str, \n",
    "                       sample_size: int = None,\n",
    "                       show_plots: bool = True) -> Dict[str, Any]:\n",
    "  \n",
    "    \n",
    "    # Load data\n",
    "    print(f\"Loading data from: {csv_file_path}\")\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    \n",
    "    # Sample if requested\n",
    "    if sample_size and len(df) > sample_size:\n",
    "        print(f\"ðŸ“Š Sampling {sample_size:,} rows from {len(df):,} total rows\")\n",
    "        df = df.sample(sample_size, random_state=42)\n",
    "    \n",
    "    # Auto-detect columns for Employee Turnover Dataset\n",
    "    numeric_cols = ['EmployeeNumber', 'Age', 'Tenure', 'HoursWeekly', \n",
    "                   'AnnualSalary', 'DrivingCommuterDistance', \n",
    "                   'NumCompaniesPreviouslyWorked', 'AnnualProfessionalDevHrs']\n",
    "    \n",
    "    categorical_cols = ['Turnover', 'HourlyRate ', 'CompensationType', \n",
    "                       'JobRoleArea', 'Gender', 'MaritalStatus', \n",
    "                       'PaycheckMethod', 'TextMessageOptIn']\n",
    "    \n",
    "    # Filter to only existing columns\n",
    "    numeric_cols = [col for col in numeric_cols if col in df.columns]\n",
    "    categorical_cols = [col for col in categorical_cols if col in df.columns]\n",
    "    \n",
    "    # Run inspection\n",
    "    report = inspect_data_quality(\n",
    "        df, \n",
    "        numeric_columns=numeric_cols,\n",
    "        categorical_columns=categorical_cols,\n",
    "        outlier_method='iqr',\n",
    "        outlier_threshold=1.5,\n",
    "        show_plots=show_plots\n",
    "    )\n",
    "    \n",
    "    return report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03e95e5d-3bf2-4058-8bd8-93822e34ab7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â“ 6. HANDLING MISSING VALUES...\n",
      "   ðŸŽ¯ Imputing 665 missing values in NumCompaniesPreviouslyWorked using mode...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cleaning_log' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 203\u001b[0m\n\u001b[1;32m    200\u001b[0m                 fill_value \u001b[38;5;241m=\u001b[39m strategy  \u001b[38;5;66;03m# <-- THIS IS THE KEY FIX\u001b[39;00m\n\u001b[1;32m    202\u001b[0m             df[col] \u001b[38;5;241m=\u001b[39m df[col]\u001b[38;5;241m.\u001b[39mfillna(fill_value)\n\u001b[0;32m--> 203\u001b[0m             \u001b[43mcleaning_log\u001b[49m\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImputed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m missing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m values with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstrategy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    204\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   âœ“ Imputed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m missing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    206\u001b[0m \u001b[38;5;66;03m# 7. FINAL VALIDATION (this needs to be indented properly)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cleaning_log' is not defined"
     ]
    }
   ],
   "source": [
    "def clean_employee_data(df: pd.DataFrame, \n",
    "                       report: Dict[str, Any] = None,\n",
    "                       backup: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "     Data cleaning function for Employee Turnover dataset\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe to clean\n",
    "    report : Dict[str, Any], optional\n",
    "        Quality report from inspect_data_quality() for targeted cleaning\n",
    "    backup : bool, default True\n",
    "        Whether to create a backup of original data\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : Cleaned dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "   #  print(\"STARTING DATA CLEANING PROCESS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create backup if requested\n",
    "    if backup:\n",
    "        df_original = df.copy()\n",
    "        print(\"Original data backed up\")\n",
    "    \n",
    "    df_cleaned = df.copy()\n",
    "    cleaning_log = []\n",
    "    \n",
    "    # 1. REMOVE DUPLICATE ROWS\n",
    "    print(\"\\n 1. REMOVING DUPLICATE ROWS...\")\n",
    "    initial_rows = len(df_cleaned)\n",
    "    df_cleaned = df_cleaned.drop_duplicates()\n",
    "    duplicates_removed = initial_rows - len(df_cleaned)\n",
    "    \n",
    "    if duplicates_removed > 0:\n",
    "        cleaning_log.append(f\"Removed {duplicates_removed} duplicate rows\")\n",
    "        print(f\"Removed {duplicates_removed} duplicate rows\")\n",
    "    else:\n",
    "        print(\"No duplicate rows found\")\n",
    "    \n",
    "    # 2. FIX DATA TYPE MISMATCHES\n",
    "    print(\"\\n 2. FIXING DATA TYPE MISMATCHES...\")\n",
    "    \n",
    "    # Fix HourlyRate column (remove $ and convert to numeric)\n",
    "    if 'HourlyRate ' in df_cleaned.columns:  # Note the space\n",
    "        print(\"Cleaning HourlyRate column...\")\n",
    "        original_col = df_cleaned['HourlyRate '].copy()\n",
    "        \n",
    "        # Remove currency symbols and extra spaces, convert to numeric\n",
    "        cleaned_values = (df_cleaned['HourlyRate ']\n",
    "                         .astype(str)\n",
    "                         .str.replace(r'[\\$Â£â‚¬Â¥,\\s]', '', regex=True)\n",
    "                         .replace('', np.nan))\n",
    "        \n",
    "        df_cleaned['HourlyRate '] = pd.to_numeric(cleaned_values, errors='coerce')\n",
    "        \n",
    "        converted_count = (~df_cleaned['HourlyRate '].isna()).sum()\n",
    "        cleaning_log.append(f\"Converted HourlyRate to numeric: {converted_count} values\")\n",
    "        print(f\"   âœ“ Converted {converted_count} HourlyRate values to numeric\")\n",
    "    \n",
    "    # Fix any other currency/numeric columns\n",
    "    currency_columns = ['AnnualSalary']  # Add other currency columns as needed\n",
    "    for col in currency_columns:\n",
    "        if col in df_cleaned.columns and df_cleaned[col].dtype == 'object':\n",
    "            print(f\"Cleaning {col} column...\")\n",
    "            cleaned_values = (df_cleaned[col]\n",
    "                             .astype(str)\n",
    "                             .str.replace(r'[\\$Â£â‚¬Â¥,\\s]', '', regex=True)\n",
    "                             .replace('', np.nan))\n",
    "            \n",
    "            df_cleaned[col] = pd.to_numeric(cleaned_values, errors='coerce')\n",
    "            converted_count = (~df_cleaned[col].isna()).sum()\n",
    "            cleaning_log.append(f\"Converted {col} to numeric: {converted_count} values\")\n",
    "            print(f\"   âœ“ Converted {converted_count} {col} values to numeric\")\n",
    "    \n",
    "    # 3. STANDARDIZE CATEGORICAL VALUES\n",
    "    print(\"\\n3. STANDARDIZING CATEGORICAL VALUES...\")\n",
    "    \n",
    "    # PaycheckMethod standardization\n",
    "    if 'PaycheckMethod' in df_cleaned.columns:\n",
    "        print(\"Standardizing PaycheckMethod...\")\n",
    "        paycheck_mapping = {\n",
    "            'Mailed Check': 'Mail Check',\n",
    "            'Mail_Check': 'Mail Check', \n",
    "            'MailedCheck': 'Mail Check',\n",
    "            'DirectDeposit': 'Direct Deposit',\n",
    "            'Direct_Deposit': 'Direct Deposit'\n",
    "        }\n",
    "        \n",
    "        original_unique = df_cleaned['PaycheckMethod'].nunique()\n",
    "        df_cleaned['PaycheckMethod'] = df_cleaned['PaycheckMethod'].replace(paycheck_mapping)\n",
    "        new_unique = df_cleaned['PaycheckMethod'].nunique()\n",
    "        \n",
    "        cleaning_log.append(f\"Standardized PaycheckMethod: {original_unique} â†’ {new_unique} unique values\")\n",
    "        print(f\"Standardized PaycheckMethod: {original_unique} â†’ {new_unique} unique values\")\n",
    "    \n",
    "    # JobRoleArea standardization\n",
    "    if 'JobRoleArea' in df_cleaned.columns:\n",
    "        print(\"Standardizing JobRoleArea...\")\n",
    "        jobrole_mapping = {\n",
    "            'Information_Technology': 'Information Technology',\n",
    "            'InformationTechnology': 'Information Technology',\n",
    "            'Human_Resources': 'Human Resources',\n",
    "            'HumanResources': 'Human Resources'\n",
    "        }\n",
    "        \n",
    "        original_unique = df_cleaned['JobRoleArea'].nunique()\n",
    "        df_cleaned['JobRoleArea'] = df_cleaned['JobRoleArea'].replace(jobrole_mapping)\n",
    "        new_unique = df_cleaned['JobRoleArea'].nunique()\n",
    "        \n",
    "        cleaning_log.append(f\"Standardized JobRoleArea: {original_unique} â†’ {new_unique} unique values\")\n",
    "        print(f\"   âœ“ Standardized JobRoleArea: {original_unique} â†’ {new_unique} unique values\")\n",
    "    \n",
    "    # 4. CLEAN WHITESPACE ISSUES\n",
    "    print(\"\\n 4. CLEANING WHITESPACE ISSUES...\")\n",
    "    \n",
    "    text_columns = df_cleaned.select_dtypes(include=['object']).columns\n",
    "    total_whitespace_cleaned = 0\n",
    "    \n",
    "    for col in text_columns:\n",
    "        if df_cleaned[col].dtype == 'object':\n",
    "            # Clean leading/trailing whitespace\n",
    "            before_clean = df_cleaned[col].astype(str)\n",
    "            after_clean = before_clean.str.strip()\n",
    "            \n",
    "            # Count changes\n",
    "            changes = (before_clean != after_clean).sum()\n",
    "            if changes > 0:\n",
    "                df_cleaned[col] = after_clean\n",
    "                total_whitespace_cleaned += changes\n",
    "                print(f\"   âœ“ Cleaned whitespace in {col}: {changes} values\")\n",
    "    \n",
    "    if total_whitespace_cleaned > 0:\n",
    "        cleaning_log.append(f\"Cleaned whitespace issues: {total_whitespace_cleaned} values\")\n",
    "    else:\n",
    "        print(\"No whitespace issues found\")\n",
    "    \n",
    "    # 5. HANDLE ABSURD VALUES\n",
    "    print(\"\\n 5. HANDLING ABSURD VALUES...\")\n",
    "    \n",
    "    # Remove negative salaries (impossible values)\n",
    "    if 'AnnualSalary' in df_cleaned.columns:\n",
    "        negative_salaries = (df_cleaned['AnnualSalary'] < 0).sum()\n",
    "        if negative_salaries > 0:\n",
    "            print(f\"Removing {negative_salaries} negative salary values...\")\n",
    "            df_cleaned = df_cleaned[df_cleaned['AnnualSalary'] >= 0]\n",
    "            cleaning_log.append(f\"Removed {negative_salaries} negative salary values\")\n",
    "            print(f\"Removed {negative_salaries} impossible negative salaries\")\n",
    "    \n",
    "    # Handle other absurd values based on business rules\n",
    "    # Age values outside reasonable range\n",
    "    if 'Age' in df_cleaned.columns:\n",
    "        absurd_ages = ((df_cleaned['Age'] < 16) | (df_cleaned['Age'] > 100)).sum()\n",
    "        if absurd_ages > 0:\n",
    "            print(f\" Flagging {absurd_ages} absurd age values...\")\n",
    "    \n",
    "            df_cleaned.loc[(df_cleaned['Age'] < 16) | (df_cleaned['Age'] > 100), 'Age'] = np.nan\n",
    "            cleaning_log.append(f\"Set {absurd_ages} absurd age values to NaN for review\")\n",
    "            print(f\"   âœ“ Set {absurd_ages} absurd age values to NaN for manual review\")\n",
    "    \n",
    "    # Negative commute distances\n",
    "    if 'DrivingCommuterDistance' in df_cleaned.columns:\n",
    "        negative_distances = (df_cleaned['DrivingCommuterDistance'] < 0).sum()\n",
    "        if negative_distances > 0:\n",
    "            print(f\"Handling {negative_distances} negative commute distances...\")\n",
    "            # Set negative distances to NaN (likely data entry errors)\n",
    "            df_cleaned.loc[df_cleaned['DrivingCommuterDistance'] < 0, 'DrivingCommuterDistance'] = np.nan\n",
    "            cleaning_log.append(f\"Set {negative_distances} negative commute distances to NaN\")\n",
    "            print(f\"   âœ“ Set {negative_distances} negative distances to NaN for review\")\n",
    "    \n",
    "    print(\"\\nâ“ 6. HANDLING MISSING VALUES...\")\n",
    "    \n",
    "# 6. HANDLE MISSING VALUES\n",
    "print(\"\\nâ“ 6. HANDLING MISSING VALUES...\")\n",
    "\n",
    "# Strategy depends on column and business context\n",
    "missing_strategies = {\n",
    "    'Age': 'median',  # Use median age\n",
    "    'DrivingCommuterDistance': 'median',  # Use median distance\n",
    "    'NumCompaniesPreviouslyWorked': 'mode',  # Use most common value\n",
    "    'AnnualProfessionalDevHrs': 'median',  # Use median hours\n",
    "    'TextMessageOptIn': 'No'  # Add no to field\n",
    "}\n",
    "\n",
    "for col, strategy in missing_strategies.items():\n",
    "    if col in df.columns:\n",
    "        missing_count = df[col].isna().sum()\n",
    "        if missing_count > 0:\n",
    "            print(f\"   ðŸŽ¯ Imputing {missing_count} missing values in {col} using {strategy}...\")\n",
    "            \n",
    "            if strategy == 'median':\n",
    "                fill_value = df[col].median()\n",
    "            elif strategy == 'mode':\n",
    "                fill_value = df[col].mode().iloc[0] if not df[col].mode().empty else 0\n",
    "            else:\n",
    "                # Handle custom string values like 'No'\n",
    "                fill_value = strategy  # <-- THIS IS THE KEY FIX\n",
    "            \n",
    "            df[col] = df[col].fillna(fill_value)\n",
    "            cleaning_log.append(f\"Imputed {missing_count} missing {col} values with {strategy}\")\n",
    "            print(f\"   âœ“ Imputed {missing_count} missing {col} values\")\n",
    "\n",
    "# 7. FINAL VALIDATION (this needs to be indented properly)\n",
    "print(\"\\n 7. FINAL VALIDATION...\")\n",
    "\n",
    "# Check data types\n",
    "numeric_cols = ['EmployeeNumber', 'Age', 'Tenure', 'HoursWeekly', \n",
    "               'AnnualSalary', 'DrivingCommuterDistance', \n",
    "               'NumCompaniesPreviouslyWorked', 'AnnualProfessionalDevHrs']\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if col in df.columns and not pd.api.types.is_numeric_dtype(df[col]):\n",
    "        print(f\"âš ï¸  Warning: {col} is still not numeric after cleaning\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ðŸ“Š CLEANING SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Original rows: {len(df):,}\")\n",
    "print(f\"Cleaned rows: {len(df_cleaned):,}\")\n",
    "print(f\"Rows removed: {len(df) - len(df_cleaned):,}\")\n",
    "\n",
    "print(f\"\\nðŸ”§ Actions taken:\")\n",
    "for action in cleaning_log:\n",
    "    print(f\"   â€¢ {action}\")\n",
    "\n",
    "print(\"\\nâœ… Data cleaning completed!\")\n",
    "\n",
    "# Store cleaning log in dataframe attributes for reference\n",
    "df.cleaning_log = cleaning_log\n",
    "\n",
    "return df  # <-- This should b\n",
    "\n",
    "def validate_cleaned_data(df_original: pd.DataFrame, df_cleaned: pd.DataFrame) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Validate the cleaned data and provide before/after comparison\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_original : pd.DataFrame\n",
    "        Original uncleaned dataframe\n",
    "    df_cleaned : pd.DataFrame\n",
    "        Cleaned dataframe\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Dict[str, Any] : Validation report\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\nVALIDATION REPORT\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    validation_report = {\n",
    "        'rows_before': len(df_original),\n",
    "        'rows_after': len(df_cleaned),\n",
    "        'columns_before': len(df_original.columns),\n",
    "        'columns_after': len(df_cleaned.columns),\n",
    "        'data_types_changed': {},\n",
    "        'missing_values_before': {},\n",
    "        'missing_values_after': {},\n",
    "        'unique_values_changed': {}\n",
    "    }\n",
    "    \n",
    "    # Data type changes\n",
    "    for col in df_cleaned.columns:\n",
    "        if col in df_original.columns:\n",
    "            if df_original[col].dtype != df_cleaned[col].dtype:\n",
    "                validation_report['data_types_changed'][col] = {\n",
    "                    'before': str(df_original[col].dtype),\n",
    "                    'after': str(df_cleaned[col].dtype)\n",
    "                }\n",
    "                print(f\" {col}: {df_original[col].dtype} â†’ {df_cleaned[col].dtype}\")\n",
    "    \n",
    "    # Missing values comparison\n",
    "    print(f\"\\n Missing Values Comparison:\")\n",
    "    for col in df_cleaned.columns:\n",
    "        if col in df_original.columns:\n",
    "            missing_before = df_original[col].isna().sum()\n",
    "            missing_after = df_cleaned[col].isna().sum()\n",
    "            \n",
    "            if missing_before != missing_after:\n",
    "                validation_report['missing_values_before'][col] = missing_before\n",
    "                validation_report['missing_values_after'][col] = missing_after\n",
    "                print(f\"   {col}: {missing_before} â†’ {missing_after}\")\n",
    "    \n",
    "    # Unique values comparison (for categorical columns)\n",
    "    print(f\"\\n Unique Values Comparison:\")\n",
    "    categorical_cols = df_cleaned.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_cols:\n",
    "        if col in df_original.columns:\n",
    "            unique_before = df_original[col].nunique()\n",
    "            unique_after = df_cleaned[col].nunique()\n",
    "            \n",
    "            if unique_before != unique_after:\n",
    "                validation_report['unique_values_changed'][col] = {\n",
    "                    'before': unique_before,\n",
    "                    'after': unique_after\n",
    "                }\n",
    "                print(f\"   {col}: {unique_before} â†’ {unique_after} unique values\")\n",
    "    \n",
    "    return validation_report\n",
    "\n",
    "def detect_salary_inconsistencies(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Detect inconsistencies between HourlyRate, HoursWeekly, and AnnualSalary\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe with salary-related columns\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : Report of inconsistent records\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"DETECTING SALARY CALCULATION INCONSISTENCIES\")\n",
    "    print(\"=\" * 55)\n",
    "    \n",
    "    # Clean column name (remove trailing space)\n",
    "    hourly_col = 'HourlyRate ' if 'HourlyRate ' in df.columns else 'HourlyRate'\n",
    "    \n",
    "    # Check if required columns exist\n",
    "    required_cols = [hourly_col, 'HoursWeekly', 'AnnualSalary']\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    \n",
    "    if missing_cols:\n",
    "        print(f\"Missing columns: {missing_cols}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Create working dataframe with complete data\n",
    "    df_complete = df[required_cols].dropna()\n",
    "    print(f\"Analyzing {len(df_complete):,} records with complete salary data\")\n",
    "    \n",
    "    # Calculate expected annual salary from hourly rate and weekly hours\n",
    "    expected_annual = df_complete[hourly_col] * df_complete['HoursWeekly'] * 52\n",
    "    actual_annual = df_complete['AnnualSalary']\n",
    "    \n",
    "    # Calculate percentage difference\n",
    "    pct_difference = abs((expected_annual - actual_annual) / expected_annual) * 100\n",
    "    \n",
    "    # Flag significant inconsistencies (>10% difference)\n",
    "    inconsistent_mask = pct_difference > 10\n",
    "    inconsistencies = df_complete[inconsistent_mask].copy()\n",
    "    \n",
    "    if len(inconsistencies) > 0:\n",
    "        # Add calculated fields to inconsistencies report\n",
    "        inconsistencies['Expected_Annual_Salary'] = expected_annual[inconsistent_mask]\n",
    "        inconsistencies['Actual_Annual_Salary'] = actual_annual[inconsistent_mask]\n",
    "        inconsistencies['Difference_Amount'] = (expected_annual[inconsistent_mask] - \n",
    "                                               actual_annual[inconsistent_mask])\n",
    "        inconsistencies['Difference_Percentage'] = pct_difference[inconsistent_mask]\n",
    "        \n",
    "        print(f\"Found {len(inconsistencies):,} inconsistent salary records\")\n",
    "        print(f\"   ({len(inconsistencies)/len(df_complete)*100:.1f}% of complete records)\")\n",
    "        \n",
    "        # Show examples of the most problematic cases\n",
    "        worst_cases = inconsistencies.nlargest(5, 'Difference_Percentage')\n",
    "        \n",
    "        print(f\"\\nTop 5 Most Inconsistent Records:\")\n",
    "        print(\"-\" * 80)\n",
    "        for idx, row in worst_cases.iterrows():\n",
    "            print(f\"Row {idx}:\")\n",
    "            print(f\"Hourly Rate: ${row[hourly_col]:.2f}/hr\")\n",
    "            print(f\"Hours/Week: {row['HoursWeekly']}\")\n",
    "            print(f\"Expected Annual: ${row['Expected_Annual_Salary']:,.2f}\")\n",
    "            print(f\"Actual Annual: ${row['Actual_Annual_Salary']:,.2f}\")\n",
    "            print(f\"Difference: ${row['Difference_Amount']:,.2f} ({row['Difference_Percentage']:.1f}%)\")\n",
    "            print()\n",
    "        \n",
    "        # Analyze patterns in the inconsistencies\n",
    "        print(f\"INCONSISTENCY PATTERNS:\")\n",
    "        \n",
    "        # Pattern 1: Low annual salary with reasonable hourly rate (your observation)\n",
    "        low_annual_high_hourly = inconsistencies[\n",
    "            (inconsistencies['Actual_Annual_Salary'] < 15000) & \n",
    "            (inconsistencies[hourly_col] > 15)\n",
    "        ]\n",
    "        \n",
    "        if len(low_annual_high_hourly) > 0:\n",
    "            print(f\"Low Annual + High Hourly: {len(low_annual_high_hourly)} records\")\n",
    "            print(f\"Example: ${low_annual_high_hourly[hourly_col].iloc[0]:.2f}/hr â†’ \"\n",
    "                  f\"${low_annual_high_hourly['Actual_Annual_Salary'].iloc[0]:,.2f}/year\")\n",
    "        \n",
    "        # Pattern 2: High annual salary with low hourly rate\n",
    "        high_annual_low_hourly = inconsistencies[\n",
    "            (inconsistencies['Actual_Annual_Salary'] > 80000) & \n",
    "            (inconsistencies[hourly_col] < 20)\n",
    "        ]\n",
    "        \n",
    "        if len(high_annual_low_hourly) > 0:\n",
    "            print(f\"High Annual + Low Hourly: {len(high_annual_low_hourly)} records\")\n",
    "        \n",
    "        # Pattern 3: Unusual weekly hours\n",
    "        unusual_hours = inconsistencies[\n",
    "            (inconsistencies['HoursWeekly'] < 20) | \n",
    "            (inconsistencies['HoursWeekly'] > 60)\n",
    "        ]\n",
    "        \n",
    "        if len(unusual_hours) > 0:\n",
    "            print(f\"Unusual Weekly Hours: {len(unusual_hours)} records\")\n",
    "            print(f\"      Hours range: {unusual_hours['HoursWeekly'].min()} - \"\n",
    "                  f\"{unusual_hours['HoursWeekly'].max()}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No significant salary inconsistencies found\")\n",
    "        inconsistencies = pd.DataFrame()\n",
    "    \n",
    "    return inconsistencies\n",
    "\n",
    "def fix_salary_inconsistencies(df: pd.DataFrame, \n",
    "                              strategy: str = 'recalculate_annual',\n",
    "                              trust_source: str = 'hourly_rate') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fix salary inconsistencies using various strategies\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe\n",
    "    strategy : str, default 'recalculate_annual'\n",
    "        Fix strategy: 'recalculate_annual', 'recalculate_hourly', 'flag_only', 'manual_review'\n",
    "    trust_source : str, default 'hourly_rate'\n",
    "        Which field to trust: 'hourly_rate', 'annual_salary', 'hours_weekly'\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : Dataframe with fixes applied\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"FIXING SALARY INCONSISTENCIES\")\n",
    "    print(f\"Strategy: {strategy} | Trust source: {trust_source}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    df_fixed = df.copy()\n",
    "    hourly_col = 'HourlyRate ' if 'HourlyRate ' in df.columns else 'HourlyRate'\n",
    "    \n",
    "    # Detect inconsistencies first\n",
    "    inconsistencies = detect_salary_inconsistencies(df_fixed)\n",
    "    \n",
    "    if len(inconsistencies) == 0:\n",
    "        print(\" No inconsistencies to fix\")\n",
    "        return df_fixed\n",
    "    \n",
    "    inconsistent_indices = inconsistencies.index\n",
    "    fixes_applied = 0\n",
    "    \n",
    "    if strategy == 'recalculate_annual':\n",
    "        print(f\"Recalculating Annual Salary from Hourly Rate Ã— Hours Ã— 52...\")\n",
    "        \n",
    "        # Calculate new annual salary\n",
    "        new_annual = (df_fixed.loc[inconsistent_indices, hourly_col] * \n",
    "                     df_fixed.loc[inconsistent_indices, 'HoursWeekly'] * 52)\n",
    "        \n",
    "        # Apply the fix\n",
    "        df_fixed.loc[inconsistent_indices, 'AnnualSalary'] = new_annual\n",
    "        fixes_applied = len(inconsistent_indices)\n",
    "        \n",
    "        print(f\"Recalculated {fixes_applied} annual salaries\")\n",
    "        \n",
    "        # Show examples of fixes\n",
    "        sample_fixes = inconsistencies.head(3)\n",
    "        for idx, row in sample_fixes.iterrows():\n",
    "            old_annual = row['Actual_Annual_Salary']\n",
    "            new_annual_val = df_fixed.loc[idx, 'AnnualSalary']\n",
    "            print(f\"   Row {idx}: ${old_annual:,.0f} â†’ ${new_annual_val:,.0f}\")\n",
    "    \n",
    "    elif strategy == 'recalculate_hourly':\n",
    "        print(f\"Recalculating Hourly Rate from Annual Salary Ã· Hours Ã· 52...\")\n",
    "        \n",
    "        # Calculate new hourly rate\n",
    "        new_hourly = (df_fixed.loc[inconsistent_indices, 'AnnualSalary'] / \n",
    "                     (df_fixed.loc[inconsistent_indices, 'HoursWeekly'] * 52))\n",
    "        \n",
    "        # Apply the fix\n",
    "        df_fixed.loc[inconsistent_indices, hourly_col] = new_hourly\n",
    "        fixes_applied = len(inconsistent_indices)\n",
    "        \n",
    "        print(f\"Recalculated {fixes_applied} hourly rates\")\n",
    "    \n",
    "    elif strategy == 'flag_only':\n",
    "        print(f\" Flagging inconsistent records without changing values...\")\n",
    "        \n",
    "        # Add flag column\n",
    "        df_fixed['Salary_Inconsistency_Flag'] = False\n",
    "        df_fixed.loc[inconsistent_indices, 'Salary_Inconsistency_Flag'] = True\n",
    "        \n",
    "        # Add details about the inconsistency\n",
    "        df_fixed['Expected_Annual_Salary'] = np.nan\n",
    "        df_fixed.loc[inconsistent_indices, 'Expected_Annual_Salary'] = (\n",
    "            df_fixed.loc[inconsistent_indices, hourly_col] * \n",
    "            df_fixed.loc[inconsistent_indices, 'HoursWeekly'] * 52\n",
    "        )\n",
    "        \n",
    "        fixes_applied = len(inconsistent_indices)\n",
    "        print(f\"Flagged {fixes_applied} inconsistent records\")\n",
    "    \n",
    "    elif strategy == 'manual_review':\n",
    "        print(f\"Creating manual review file...\")\n",
    "        \n",
    "        # Export detailed inconsistency report\n",
    "        review_data = inconsistencies.copy()\n",
    "        review_file = 'salary_inconsistencies_for_review.csv'\n",
    "        review_data.to_csv(review_file, index=True)\n",
    "        \n",
    "        print(f\"Exported {len(inconsistencies)} records to {review_file}\")\n",
    "        print(f\"Manual review required - no automatic fixes applied\")\n",
    "        \n",
    "        return df_fixed\n",
    "    \n",
    "    print(f\"\\n FIXING SUMMARY:\")\n",
    "    print(f\"Records processed: {len(inconsistencies):,}\")\n",
    "    print(f\"Fixes applied: {fixes_applied:,}\")\n",
    "    print(f\"Strategy used: {strategy}\")\n",
    "    \n",
    "    return df_fixed\n",
    "\n",
    "def validate_salary_fixes(df_original: pd.DataFrame, df_fixed: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Validate that salary fixes resolved the inconsistencies\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_original : pd.DataFrame\n",
    "        Original dataframe before fixes\n",
    "    df_fixed : pd.DataFrame\n",
    "        Dataframe after fixes applied\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"VALIDATING SALARY FIXES\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Check inconsistencies before and after\n",
    "    inconsistencies_before = detect_salary_inconsistencies(df_original)\n",
    "    inconsistencies_after = detect_salary_inconsistencies(df_fixed)\n",
    "    \n",
    "    print(f\"Inconsistencies before: {len(inconsistencies_before):,}\")\n",
    "    print(f\"Inconsistencies after: {len(inconsistencies_after):,}\")\n",
    "    print(f\"Improvement: {len(inconsistencies_before) - len(inconsistencies_after):,} records fixed\")\n",
    "    \n",
    "    if len(inconsistencies_after) == 0:\n",
    "        print(\"All salary inconsistencies resolved!\")\n",
    "    else:\n",
    "        print(f\"{len(inconsistencies_after)} inconsistencies remain\")\n",
    "\n",
    "def inspect_salary_math_consistency(df: pd.DataFrame, \n",
    "                                   tolerance_percentage: float = 10.0,\n",
    "                                   weeks_per_year: float = 52.0) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Comprehensive salary math validation to ensure HourlyRate Ã— HoursWeekly Ã— 52 = AnnualSalary\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe with salary columns\n",
    "    tolerance_percentage : float, default 10.0\n",
    "        Acceptable percentage difference between calculated and actual annual salary\n",
    "    weeks_per_year : float, default 52.0\n",
    "        Number of work weeks per year (adjust for vacation/holidays if needed)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Dict[str, Any] : Comprehensive salary consistency report\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"SALARY MATH CONSISTENCY INSPECTION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Handle column name variations\n",
    "    hourly_col = None\n",
    "    for col in ['HourlyRate', 'HourlyRate ', 'Hourly Rate', 'hourly_rate']:\n",
    "        if col in df.columns:\n",
    "            hourly_col = col\n",
    "            break\n",
    "    \n",
    "    if hourly_col is None:\n",
    "        print(\"No hourly rate column found\")\n",
    "        return {'error': 'No hourly rate column found'}\n",
    "    \n",
    "    # Check required columns\n",
    "    required_cols = [hourly_col, 'HoursWeekly', 'AnnualSalary']\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    \n",
    "    if missing_cols:\n",
    "        print(f\"Missing required columns: {missing_cols}\")\n",
    "        return {'error': f'Missing columns: {missing_cols}'}\n",
    "    \n",
    "    print(f\"Using columns: {hourly_col}, HoursWeekly, AnnualSalary\")\n",
    "    print(f\"Tolerance: Â±{tolerance_percentage}%\")\n",
    "    print(f\"Work weeks per year: {weeks_per_year}\")\n",
    "    \n",
    "    # Create a working copy and clean the data for analysis\n",
    "    df_work = df.copy()\n",
    "    \n",
    "    # Clean the hourly rate column if it contains text/currency symbols\n",
    "    if df_work[hourly_col].dtype == 'object':\n",
    "        print(f\"Cleaning {hourly_col} column (removing currency symbols)...\")\n",
    "        \n",
    "        # Remove currency symbols and convert to numeric\n",
    "        cleaned_hourly = (df_work[hourly_col]\n",
    "                         .astype(str)\n",
    "                         .str.replace(r'[\\$Â£â‚¬Â¥,\\s]', '', regex=True)\n",
    "                         .replace('', np.nan))\n",
    "        \n",
    "        df_work[hourly_col] = pd.to_numeric(cleaned_hourly, errors='coerce')\n",
    "        converted_count = df_work[hourly_col].notna().sum()\n",
    "        print(f\"Converted {converted_count} hourly rate values to numeric\")\n",
    "    \n",
    "    # Ensure other salary columns are numeric\n",
    "    for col in ['HoursWeekly', 'AnnualSalary']:\n",
    "        if df_work[col].dtype == 'object':\n",
    "            print(f\"Cleaning {col} column...\")\n",
    "            cleaned_values = (df_work[col]\n",
    "                             .astype(str)\n",
    "                             .str.replace(r'[\\$Â£â‚¬Â¥,\\s]', '', regex=True)\n",
    "                             .replace('', np.nan))\n",
    "            df_work[col] = pd.to_numeric(cleaned_values, errors='coerce')\n",
    "    \n",
    "    # Create report structure\n",
    "    report = {\n",
    "        'dataset_info': {\n",
    "            'total_rows': len(df_work),\n",
    "            'complete_salary_records': 0,\n",
    "            'incomplete_records': 0\n",
    "        },\n",
    "        'math_consistency': {\n",
    "            'consistent_records': 0,\n",
    "            'inconsistent_records': 0,\n",
    "            'consistency_percentage': 0.0\n",
    "        },\n",
    "        'inconsistency_details': {\n",
    "            'by_severity': {},\n",
    "            'by_pattern': {},\n",
    "            'worst_cases': [],\n",
    "            'statistical_summary': {}\n",
    "        },\n",
    "        'flagged_records': [],\n",
    "        'recommendations': []\n",
    "    }\n",
    "    \n",
    "    # Filter to records with complete salary data (now that data is cleaned)\n",
    "    salary_mask = (\n",
    "        df_work[hourly_col].notna() & \n",
    "        df_work['HoursWeekly'].notna() & \n",
    "        df_work['AnnualSalary'].notna() &\n",
    "        (df_work[hourly_col] > 0) &\n",
    "        (df_work['HoursWeekly'] > 0) &\n",
    "        (df_work['AnnualSalary'] > 0)\n",
    "    )\n",
    "    \n",
    "    complete_records = df_work[salary_mask].copy()\n",
    "    incomplete_count = len(df_work) - len(complete_records)\n",
    "    \n",
    "    report['dataset_info']['complete_salary_records'] = len(complete_records)\n",
    "    report['dataset_info']['incomplete_records'] = incomplete_count\n",
    "    \n",
    "    print(f\"\\nDATA OVERVIEW:\")\n",
    "    print(f\"   Total records: {len(df_work):,}\")\n",
    "    print(f\"   Complete salary data: {len(complete_records):,}\")\n",
    "    print(f\"   Incomplete/invalid: {incomplete_count:,}\")\n",
    "    \n",
    "    if len(complete_records) == 0:\n",
    "        print(\"No complete salary records to analyze\")\n",
    "        return report\n",
    "    \n",
    "    # Calculate expected annual salary\n",
    "    expected_annual = (complete_records[hourly_col] * \n",
    "                      complete_records['HoursWeekly'] * \n",
    "                      weeks_per_year)\n",
    "    \n",
    "    actual_annual = complete_records['AnnualSalary']\n",
    "    \n",
    "    # Calculate absolute percentage difference\n",
    "    percentage_diff = abs((expected_annual - actual_annual) / expected_annual) * 100\n",
    "    \n",
    "    # Identify inconsistent records\n",
    "    inconsistent_mask = percentage_diff > tolerance_percentage\n",
    "    consistent_mask = ~inconsistent_mask\n",
    "    \n",
    "    consistent_count = consistent_mask.sum()\n",
    "    inconsistent_count = inconsistent_mask.sum()\n",
    "    \n",
    "    report['math_consistency']['consistent_records'] = int(consistent_count)\n",
    "    report['math_consistency']['inconsistent_records'] = int(inconsistent_count)\n",
    "    report['math_consistency']['consistency_percentage'] = (consistent_count / len(complete_records)) * 100\n",
    "    \n",
    "    print(f\"\\nMATH CONSISTENCY RESULTS:\")\n",
    "    print(f\"  Consistent records: {consistent_count:,} ({(consistent_count/len(complete_records)*100):.1f}%)\")\n",
    "    print(f\"  Inconsistent records: {inconsistent_count:,} ({(inconsistent_count/len(complete_records)*100):.1f}%)\")\n",
    "    \n",
    "    if inconsistent_count > 0:\n",
    "        # Analyze inconsistency patterns\n",
    "        inconsistent_records = complete_records[inconsistent_mask].copy()\n",
    "        inconsistent_records['Expected_Annual'] = expected_annual[inconsistent_mask]\n",
    "        inconsistent_records['Actual_Annual'] = actual_annual[inconsistent_mask]\n",
    "        inconsistent_records['Dollar_Difference'] = expected_annual[inconsistent_mask] - actual_annual[inconsistent_mask]\n",
    "        inconsistent_records['Percentage_Difference'] = percentage_diff[inconsistent_mask]\n",
    "        \n",
    "        # Severity classification\n",
    "        severity_ranges = {\n",
    "            'Minor (10-25%)': (10, 25),\n",
    "            'Moderate (25-50%)': (25, 50),\n",
    "            'Major (50-100%)': (50, 100),\n",
    "            'Severe (>100%)': (100, float('inf'))\n",
    "        }\n",
    "        \n",
    "        severity_counts = {}\n",
    "        for severity, (min_pct, max_pct) in severity_ranges.items():\n",
    "            count = ((percentage_diff >= min_pct) & (percentage_diff < max_pct)).sum()\n",
    "            severity_counts[severity] = int(count)\n",
    "        \n",
    "        report['inconsistency_details']['by_severity'] = severity_counts\n",
    "        \n",
    "        print(f\"\\n INCONSISTENCY SEVERITY:\")\n",
    "        for severity, count in severity_counts.items():\n",
    "            if count > 0:\n",
    "                print(f\"   {severity}: {count:,} records\")\n",
    "        \n",
    "        # Pattern analysis\n",
    "        patterns = {}\n",
    "        \n",
    "        # Pattern 1: Low annual, reasonable hourly (your case)\n",
    "        low_annual_reasonable_hourly = (\n",
    "            (inconsistent_records['Actual_Annual'] < 20000) & \n",
    "            (inconsistent_records[hourly_col] > 15) &\n",
    "            (inconsistent_records['HoursWeekly'] >= 35)\n",
    "        ).sum()\n",
    "        patterns['Low Annual + High Hourly + Full Time'] = int(low_annual_reasonable_hourly)\n",
    "        \n",
    "        # Pattern 2: High annual, low hourly\n",
    "        high_annual_low_hourly = (\n",
    "            (inconsistent_records['Actual_Annual'] > 80000) & \n",
    "            (inconsistent_records[hourly_col] < 25)\n",
    "        ).sum()\n",
    "        patterns['High Annual + Low Hourly'] = int(high_annual_low_hourly)\n",
    "        \n",
    "        # Pattern 3: Unrealistic hours\n",
    "        unrealistic_hours = (\n",
    "            (inconsistent_records['HoursWeekly'] < 10) | \n",
    "            (inconsistent_records['HoursWeekly'] > 70)\n",
    "        ).sum()\n",
    "        patterns['Unrealistic Weekly Hours'] = int(unrealistic_hours)\n",
    "        \n",
    "        # Pattern 4: Possible monthly vs annual confusion\n",
    "        monthly_confusion = (\n",
    "            abs(inconsistent_records['Actual_Annual'] * 12 - inconsistent_records['Expected_Annual']) / \n",
    "            inconsistent_records['Expected_Annual'] * 100 < tolerance_percentage\n",
    "        ).sum()\n",
    "        patterns['Possible Monthly Salary Error'] = int(monthly_confusion)\n",
    "        \n",
    "        report['inconsistency_details']['by_pattern'] = patterns\n",
    "        \n",
    "        print(f\"\\nINCONSISTENCY PATTERNS:\")\n",
    "        for pattern, count in patterns.items():\n",
    "            if count > 0:\n",
    "                print(f\"   {pattern}: {count:,} records\")\n",
    "        \n",
    "        # Statistical summary of inconsistencies\n",
    "        stats = {\n",
    "            'avg_percentage_diff': float(percentage_diff[inconsistent_mask].mean()),\n",
    "            'median_percentage_diff': float(percentage_diff[inconsistent_mask].median()),\n",
    "            'max_percentage_diff': float(percentage_diff[inconsistent_mask].max()),\n",
    "            'avg_dollar_diff': float(inconsistent_records['Dollar_Difference'].mean()),\n",
    "            'median_dollar_diff': float(inconsistent_records['Dollar_Difference'].median())\n",
    "        }\n",
    "        \n",
    "        report['inconsistency_details']['statistical_summary'] = stats\n",
    "        \n",
    "        print(f\"\\nINCONSISTENCY STATISTICS:\")\n",
    "        print(f\"Average difference: {stats['avg_percentage_diff']:.1f}% (${stats['avg_dollar_diff']:,.0f})\")\n",
    "        print(f\"Median difference: {stats['median_percentage_diff']:.1f}% (${stats['median_dollar_diff']:,.0f})\")\n",
    "        print(f\"Maximum difference: {stats['max_percentage_diff']:.1f}%\")\n",
    "        \n",
    "        # Top 10 worst cases\n",
    "        worst_cases = inconsistent_records.nlargest(10, 'Percentage_Difference')\n",
    "        worst_cases_list = []\n",
    "        \n",
    "        print(f\"\\nTOP 10 WORST INCONSISTENCIES:\")\n",
    "        print(\"-\" * 80)\n",
    "        for i, (idx, row) in enumerate(worst_cases.iterrows(), 1):\n",
    "            case_info = {\n",
    "                'rank': i,\n",
    "                'row_index': int(idx),\n",
    "                'hourly_rate': float(row[hourly_col]),\n",
    "                'hours_weekly': float(row['HoursWeekly']),\n",
    "                'expected_annual': float(row['Expected_Annual']),\n",
    "                'actual_annual': float(row['Actual_Annual']),\n",
    "                'difference_pct': float(row['Percentage_Difference']),\n",
    "                'difference_dollars': float(row['Dollar_Difference'])\n",
    "            }\n",
    "            worst_cases_list.append(case_info)\n",
    "            \n",
    "            print(f\"{i:2d}. Row {idx}: ${row[hourly_col]:.2f}/hr Ã— {row['HoursWeekly']}hr/wk = \"\n",
    "                  f\"${row['Expected_Annual']:,.0f}/yr BUT shows ${row['Actual_Annual']:,.0f}/yr \"\n",
    "                  f\"({row['Percentage_Difference']:.1f}% off)\")\n",
    "        \n",
    "        report['inconsistency_details']['worst_cases'] = worst_cases_list\n",
    "        \n",
    "        # Create flagged records list for cleaning process\n",
    "        flagged_records = []\n",
    "        for idx, row in inconsistent_records.iterrows():\n",
    "            flagged_records.append({\n",
    "                'row_index': int(idx),\n",
    "                'issue_type': 'SALARY_MATH_INCONSISTENCY',\n",
    "                'severity': get_severity_level(row['Percentage_Difference']),\n",
    "                'details': f\"Expected ${row['Expected_Annual']:,.0f} but shows ${row['Actual_Annual']:,.0f} ({row['Percentage_Difference']:.1f}% difference)\",\n",
    "                'hourly_rate': float(row[hourly_col]),\n",
    "                'hours_weekly': float(row['HoursWeekly']),\n",
    "                'annual_salary': float(row['Actual_Annual']),\n",
    "                'expected_annual': float(row['Expected_Annual']),\n",
    "                'needs_review': True\n",
    "            })\n",
    "        \n",
    "        report['flagged_records'] = flagged_records\n",
    "        \n",
    "        # Generate recommendations\n",
    "        recommendations = []\n",
    "        \n",
    "        if patterns['Low Annual + High Hourly + Full Time'] > 0:\n",
    "            recommendations.append(f\"CRITICAL: {patterns['Low Annual + High Hourly + Full Time']} records have suspiciously low annual salaries despite reasonable hourly rates - likely data entry errors\")\n",
    "        \n",
    "        if patterns['Possible Monthly Salary Error'] > 0:\n",
    "            recommendations.append(f\"INVESTIGATE: {patterns['Possible Monthly Salary Error']} records might have monthly salary entered instead of annual\")\n",
    "        \n",
    "        if patterns['Unrealistic Weekly Hours'] > 0:\n",
    "            recommendations.append(f\"REVIEW: {patterns['Unrealistic Weekly Hours']} records have unusual weekly hours that may affect calculations\")\n",
    "        \n",
    "        if inconsistent_count > len(complete_records) * 0.1:  # More than 10% inconsistent\n",
    "            recommendations.append(\"HIGH PRIORITY: Large percentage of salary records are mathematically inconsistent - review data collection process\")\n",
    "        \n",
    "        recommendations.append(f\"RECOMMENDED ACTION: Flag all {inconsistent_count:,} inconsistent records for manual review before analysis\")\n",
    "        \n",
    "        report['recommendations'] = recommendations\n",
    "        \n",
    "        print(f\"\\n RECOMMENDATIONS:\")\n",
    "        for rec in recommendations:\n",
    "            print(f\"   {rec}\")\n",
    "    \n",
    "    print(f\"\\n Salary math inspection completed!\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    return report\n",
    "\n",
    "def get_severity_level(percentage_diff: float) -> str:\n",
    "    \"\"\"Helper function to classify inconsistency severity\"\"\"\n",
    "    if percentage_diff < 25:\n",
    "        return 'MINOR'\n",
    "    elif percentage_diff < 50:\n",
    "        return 'MODERATE'\n",
    "    elif percentage_diff < 100:\n",
    "        return 'MAJOR'\n",
    "    else:\n",
    "        return 'SEVERE'\n",
    "\n",
    "def add_salary_consistency_flags(df: pd.DataFrame, \n",
    "                                salary_report: Dict[str, Any]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add salary consistency flags to the dataframe based on inspection results\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe\n",
    "    salary_report : Dict[str, Any]\n",
    "        Report from inspect_salary_math_consistency()\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : Dataframe with added flag columns\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"ADDING SALARY CONSISTENCY FLAGS TO DATASET\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    df_flagged = df.copy()\n",
    "    \n",
    "    # Initialize flag columns\n",
    "    df_flagged['Salary_Math_Consistent'] = True\n",
    "    df_flagged['Salary_Inconsistency_Severity'] = 'NONE'\n",
    "    df_flagged['Salary_Needs_Review'] = False\n",
    "    df_flagged['Salary_Issue_Details'] = ''\n",
    "    \n",
    "    # Apply flags based on report\n",
    "    if 'flagged_records' in salary_report:\n",
    "        flagged_count = 0\n",
    "        \n",
    "        for record in salary_report['flagged_records']:\n",
    "            idx = record['row_index']\n",
    "            \n",
    "            df_flagged.loc[idx, 'Salary_Math_Consistent'] = False\n",
    "            df_flagged.loc[idx, 'Salary_Inconsistency_Severity'] = record['severity']\n",
    "            df_flagged.loc[idx, 'Salary_Needs_Review'] = True\n",
    "            df_flagged.loc[idx, 'Salary_Issue_Details'] = record['details']\n",
    "            \n",
    "            flagged_count += 1\n",
    "        \n",
    "        print(f\"   âœ“ Flagged {flagged_count:,} records with salary inconsistencies\")\n",
    "        \n",
    "        # Summary by severity\n",
    "        severity_summary = df_flagged['Salary_Inconsistency_Severity'].value_counts()\n",
    "        print(f\"\\n FLAGS BY SEVERITY:\")\n",
    "        for severity, count in severity_summary.items():\n",
    "            if severity != 'NONE':\n",
    "                print(f\"      {severity}: {count:,}\")\n",
    "    \n",
    "    else:\n",
    "        print(\"No salary inconsistencies found - no flags added\")\n",
    "    \n",
    "    return df_flagged\n",
    "\n",
    "# Usage examples:\n",
    "\"\"\"\n",
    "# Example 1: Run salary math inspection after main data quality check\n",
    "main_report = quick_quality_check('Employee Turnover Dataset.csv')\n",
    "salary_report = inspect_salary_math_consistency(df)\n",
    "\n",
    "# Example 2: Add flags to dataset for cleaning process\n",
    "df_flagged = add_salary_consistency_flags(df, salary_report)\n",
    "\n",
    "# Example 3: Export flagged records for manual review\n",
    "flagged_records = df_flagged[df_flagged['Salary_Needs_Review'] == True]\n",
    "flagged_records.to_csv('salary_inconsistencies_for_review.csv', index=False)\n",
    "\n",
    "# Example 4: Check specific tolerance levels\n",
    "strict_report = inspect_salary_math_consistency(df, tolerance_percentage=5.0)  # Stricter\n",
    "lenient_report = inspect_salary_math_consistency(df, tolerance_percentage=15.0)  # More lenient\n",
    "\n",
    "# Example 5: Adjust for different work schedules\n",
    "# 50 weeks (accounting for 2 weeks vacation)\n",
    "vacation_report = inspect_salary_math_consistency(df, weeks_per_year=50.0)\n",
    "\"\"\"\n",
    "\n",
    "def handle_outliers_and_absurd_values(df: pd.DataFrame, \n",
    "                                    method: str = 'investigate',\n",
    "                                    outlier_method: str = 'iqr',\n",
    "                                    outlier_threshold: float = 1.5,\n",
    "                                    fix_salary_inconsistencies: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Comprehensive outlier and absurd value handling with multiple strategies\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe to process\n",
    "    method : str, default 'investigate'\n",
    "        Strategy: 'investigate', 'remove', 'cap', 'transform', 'flag'\n",
    "    outlier_method : str, default 'iqr'\n",
    "        Outlier detection method: 'iqr', 'zscore', 'modified_zscore'\n",
    "    outlier_threshold : float, default 1.5\n",
    "        Threshold for outlier detection\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : Processed dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"HANDLING OUTLIERS AND ABSURD VALUES\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    df_processed = df.copy()\n",
    "    processing_log = []\n",
    "    \n",
    "    # Define numeric columns to check\n",
    "    numeric_columns = ['Age', 'Tenure', 'HoursWeekly', 'AnnualSalary', \n",
    "                      'DrivingCommuterDistance', 'NumCompaniesPreviouslyWorked', \n",
    "                      'AnnualProfessionalDevHrs']\n",
    "    \n",
    "    # 1. HANDLE ABSURD VALUES FIRST (Logically impossible)\n",
    "    print(\"\\n1. HANDLING ABSURD VALUES (Logically Impossible)...\")\n",
    "    \n",
    "    absurd_rules = {\n",
    "        'Age': {'min': 16, 'max': 100, 'reason': 'Working age limits'},\n",
    "        'Tenure': {'min': 0, 'max': 50, 'reason': 'Career length limits'},\n",
    "        'HoursWeekly': {'min': 0, 'max': 80, 'reason': 'Legal working hours'},\n",
    "        'AnnualSalary': {'min': 0, 'max': None, 'reason': 'Cannot be negative'},\n",
    "        'DrivingCommuterDistance': {'min': 0, 'max': 100, 'reason': 'Reasonable commute limits'},\n",
    "        'NumCompaniesPreviouslyWorked': {'min': 0, 'max': 20, 'reason': 'Career history limits'},\n",
    "        'AnnualProfessionalDevHrs': {'min': 0, 'max': 2000, 'reason': 'Annual time limits'}\n",
    "    }\n",
    "    \n",
    "    for col in numeric_columns:\n",
    "        if col in df_processed.columns and col in absurd_rules:\n",
    "            rules = absurd_rules[col]\n",
    "            \n",
    "            # Count absurd values\n",
    "            absurd_mask = pd.Series([False] * len(df_processed), index=df_processed.index)\n",
    "            \n",
    "            if rules['min'] is not None:\n",
    "                absurd_mask |= (df_processed[col] < rules['min'])\n",
    "            if rules['max'] is not None:\n",
    "                absurd_mask |= (df_processed[col] > rules['max'])\n",
    "                \n",
    "            absurd_count = absurd_mask.sum()\n",
    "            \n",
    "            if absurd_count > 0:\n",
    "                print(f\" {col}: {absurd_count} absurd values ({rules['reason']})\")\n",
    "                \n",
    "                if method == 'remove':\n",
    "                    df_processed = df_processed[~absurd_mask]\n",
    "                    processing_log.append(f\"Removed {absurd_count} absurd {col} values\")\n",
    "                \n",
    "                elif method == 'cap':\n",
    "                    if rules['min'] is not None:\n",
    "                        df_processed.loc[df_processed[col] < rules['min'], col] = rules['min']\n",
    "                    if rules['max'] is not None:\n",
    "                        df_processed.loc[df_processed[col] > rules['max'], col] = rules['max']\n",
    "                    processing_log.append(f\"Capped {absurd_count} absurd {col} values to valid range\")\n",
    "                \n",
    "                elif method == 'flag':\n",
    "                    df_processed[f'{col}_absurd_flag'] = absurd_mask\n",
    "                    processing_log.append(f\"Flagged {absurd_count} absurd {col} values\")\n",
    "                \n",
    "                else:  # investigate (default)\n",
    "                    df_processed.loc[absurd_mask, col] = np.nan\n",
    "                    processing_log.append(f\"Set {absurd_count} absurd {col} values to NaN for investigation\")\n",
    "                \n",
    "                print(f\"      â†’ Action taken: {method}\")\n",
    "    \n",
    "    # 2. HANDLE STATISTICAL OUTLIERS\n",
    "    print(f\"\\n 2. HANDLING STATISTICAL OUTLIERS (Method: {outlier_method})...\")\n",
    "    \n",
    "    outlier_summary = {}\n",
    "    \n",
    "    for col in numeric_columns:\n",
    "        if col in df_processed.columns and pd.api.types.is_numeric_dtype(df_processed[col]):\n",
    "            col_data = df_processed[col].dropna()\n",
    "            \n",
    "            if len(col_data) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Detect outliers using specified method\n",
    "            outlier_mask = detect_outliers(col_data, method=outlier_method, threshold=outlier_threshold)\n",
    "            outlier_count = outlier_mask.sum()\n",
    "            \n",
    "            if outlier_count > 0:\n",
    "                outlier_values = col_data[outlier_mask]\n",
    "                outlier_percentage = (outlier_count / len(col_data)) * 100\n",
    "                \n",
    "                print(f\" {col}: {outlier_count} outliers ({outlier_percentage:.1f}%)\")\n",
    "                print(f\"      Range: {outlier_values.min():.2f} to {outlier_values.max():.2f}\")\n",
    "                \n",
    "                outlier_summary[col] = {\n",
    "                    'count': outlier_count,\n",
    "                    'percentage': outlier_percentage,\n",
    "                    'values': outlier_values.tolist()[:10],  # First 10 for inspection\n",
    "                    'min': outlier_values.min(),\n",
    "                    'max': outlier_values.max()\n",
    "                }\n",
    "                \n",
    "                # Apply outlier handling strategy\n",
    "                if method == 'remove':\n",
    "                    # Remove outlier rows\n",
    "                    df_processed = df_processed.loc[~col_data.index.isin(outlier_values.index)]\n",
    "                    processing_log.append(f\"Removed {outlier_count} outlier rows for {col}\")\n",
    "                \n",
    "                elif method == 'cap':\n",
    "                    # Cap to 5th and 95th percentiles\n",
    "                    lower_cap = col_data.quantile(0.05)\n",
    "                    upper_cap = col_data.quantile(0.95)\n",
    "                    \n",
    "                    df_processed.loc[df_processed[col] < lower_cap, col] = lower_cap\n",
    "                    df_processed.loc[df_processed[col] > upper_cap, col] = upper_cap\n",
    "                    processing_log.append(f\"Capped {outlier_count} {col} outliers to 5th-95th percentile range\")\n",
    "                \n",
    "                elif method == 'transform':\n",
    "                    # Log transformation for right-skewed data\n",
    "                    if col_data.min() > 0:  # Only if all values are positive\n",
    "                        df_processed[f'{col}_log'] = np.log1p(df_processed[col])\n",
    "                        processing_log.append(f\"Created log-transformed {col}_log for {col}\")\n",
    "                    else:\n",
    "                        print(f\"      â†’ Cannot log-transform {col} (contains non-positive values)\")\n",
    "                \n",
    "                elif method == 'flag':\n",
    "                    # Create outlier flag column\n",
    "                    df_processed[f'{col}_outlier_flag'] = False\n",
    "                    df_processed.loc[col_data.index[outlier_mask], f'{col}_outlier_flag'] = True\n",
    "                    processing_log.append(f\"Flagged {outlier_count} {col} outliers\")\n",
    "                \n",
    "                else:  # investigate (default)\n",
    "                    print(f\"      â†’ Outliers preserved for investigation\")\n",
    "                    processing_log.append(f\"Identified {outlier_count} {col} outliers for investigation\")\n",
    "    \n",
    "    # 3. BUSINESS CONTEXT RECOMMENDATIONS\n",
    "    print(f\"\\n3. BUSINESS CONTEXT RECOMMENDATIONS...\")\n",
    "    \n",
    "    business_recommendations = {\n",
    "        'Age': \"Low ages (16-20) might be student workers - verify legitimacy\",\n",
    "        'AnnualSalary': \"Very low salaries might be part-time or intern positions\",\n",
    "        'DrivingCommuterDistance': \"High distances (>100 miles) might indicate remote workers\",\n",
    "        'HoursWeekly': \"Low hours might indicate part-time or contractor status\",\n",
    "        'AnnualProfessionalDevHrs': \"Zero dev hours might indicate entry-level positions\"\n",
    "    }\n",
    "    \n",
    "    for col, rec in business_recommendations.items():\n",
    "        if col in outlier_summary:\n",
    "            print(f\"    {col}: {rec}\")\n",
    "    \n",
    "    # 4. SUMMARY REPORT\n",
    "    print(f\"\\n 4. PROCESSING SUMMARY...\")\n",
    "    print(f\"Strategy used: {method}\")\n",
    "    print(f\"Original rows: {len(df):,}\")\n",
    "    print(f\"Processed rows: {len(df_processed):,}\")\n",
    "    \n",
    "    if processing_log:\n",
    "        print(f\"\\n Actions taken:\")\n",
    "        for action in processing_log:\n",
    "            print(f\"   â€¢ {action}\")\n",
    "    \n",
    "    # Store processing info\n",
    "    df_processed.outlier_processing_log = processing_log\n",
    "    df_processed.outlier_summary = outlier_summary\n",
    "    \n",
    "    return df_processed\n",
    "\n",
    "def detect_outliers(data: pd.Series, \n",
    "                   method: str = 'iqr', \n",
    "                   threshold: float = 1.5) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Detect outliers using various statistical methods\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pd.Series\n",
    "        Numeric data to analyze\n",
    "    method : str, default 'iqr'\n",
    "        Detection method: 'iqr', 'zscore', 'modified_zscore'\n",
    "    threshold : float, default 1.5\n",
    "        Threshold for outlier detection\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.Series : Boolean mask indicating outliers\n",
    "    \"\"\"\n",
    "    \n",
    "    if method == 'iqr':\n",
    "        Q1 = data.quantile(0.25)\n",
    "        Q3 = data.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - threshold * IQR\n",
    "        upper_bound = Q3 + threshold * IQR\n",
    "        return (data < lower_bound) | (data > upper_bound)\n",
    "    \n",
    "    elif method == 'zscore':\n",
    "        z_scores = np.abs((data - data.mean()) / data.std())\n",
    "        return z_scores > threshold\n",
    "    \n",
    "    elif method == 'modified_zscore':\n",
    "        median = data.median()\n",
    "        mad = np.median(np.abs(data - median))\n",
    "        if mad == 0:\n",
    "            return pd.Series([False] * len(data), index=data.index)\n",
    "        modified_z_scores = 0.6745 * (data - median) / mad\n",
    "        return np.abs(modified_z_scores) > threshold\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "def create_outlier_investigation_report(df: pd.DataFrame, \n",
    "                                       output_file: str = 'outlier_investigation.csv') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a detailed report of outliers for manual investigation\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe\n",
    "    output_file : str, default 'outlier_investigation.csv'\n",
    "        Output filename for investigation report\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : Investigation report\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"CREATING OUTLIER INVESTIGATION REPORT...\")\n",
    "    \n",
    "    investigation_data = []\n",
    "    numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    for col in numeric_columns:\n",
    "        if col in df.columns:\n",
    "            col_data = df[col].dropna()\n",
    "            \n",
    "            # Detect outliers using IQR method\n",
    "            outlier_mask = detect_outliers(col_data, method='iqr', threshold=1.5)\n",
    "            outlier_indices = col_data[outlier_mask].index\n",
    "            \n",
    "            for idx in outlier_indices:\n",
    "                investigation_data.append({\n",
    "                    'Row_Index': idx,\n",
    "                    'Column': col,\n",
    "                    'Value': df.loc[idx, col],\n",
    "                    'Column_Mean': col_data.mean(),\n",
    "                    'Column_Median': col_data.median(),\n",
    "                    'Column_Std': col_data.std(),\n",
    "                    'Z_Score': abs((df.loc[idx, col] - col_data.mean()) / col_data.std()),\n",
    "                    'Percentile': (col_data < df.loc[idx, col]).mean() * 100,\n",
    "                    'Action_Needed': 'INVESTIGATE',\n",
    "                    'Notes': ''\n",
    "                })\n",
    "    \n",
    "    investigation_df = pd.DataFrame(investigation_data)\n",
    "    \n",
    "    if len(investigation_df) > 0:\n",
    "        investigation_df.to_csv(output_file, index=False)\n",
    "        print(f\"Investigation report saved: {output_file}\")\n",
    "        print(f\"    {len(investigation_df)} outliers require investigation\")\n",
    "        \n",
    "        # Show summary by column\n",
    "        summary = investigation_df.groupby('Column').size().sort_values(ascending=False)\n",
    "        print(f\"\\nOutliers by column:\")\n",
    "        for col, count in summary.items():\n",
    "            print(f\"      â€¢ {col}: {count}\")\n",
    "    else:\n",
    "        print(\"No outliers detected\")\n",
    "    \n",
    "    return investigation_df\n",
    "\n",
    "\n",
    "def export_cleaning_report(df_cleaned: pd.DataFrame, \n",
    "                          filename: str = 'cleaning_report.txt') -> None:\n",
    "    \"\"\"\n",
    "    Export cleaning actions to a text file for documentation\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_cleaned : pd.DataFrame\n",
    "        Cleaned dataframe with cleaning_log attribute\n",
    "    filename : str, default 'cleaning_report.txt'\n",
    "        Output filename for the report\n",
    "    \"\"\"\n",
    "    \n",
    "    if hasattr(df_cleaned, 'cleaning_log'):\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(\"DATA CLEANING REPORT\\n\")\n",
    "            f.write(\"=\" * 50 + \"\\n\")\n",
    "            f.write(f\"Generated on: {pd.Timestamp.now()}\\n\\n\")\n",
    "            \n",
    "            f.write(\"ACTIONS TAKEN:\\n\")\n",
    "            f.write(\"-\" * 20 + \"\\n\")\n",
    "            for action in df_cleaned.cleaning_log:\n",
    "                f.write(f\"â€¢ {action}\\n\")\n",
    "            \n",
    "            f.write(f\"\\nFINAL DATASET:\\n\")\n",
    "            f.write(\"-\" * 20 + \"\\n\")\n",
    "            f.write(f\"Rows: {len(df_cleaned):,}\\n\")\n",
    "            f.write(f\"Columns: {len(df_cleaned.columns)}\\n\")\n",
    "            \n",
    "        print(f\"Cleaning report exported to: {filename}\")\n",
    "    else:\n",
    "        print(\"No cleaning log found in dataframe\")\n",
    "\n",
    "# Usage examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90feeaf7-eda2-4283-8e69-332de988a5f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (shap_env_new)",
   "language": "python",
   "name": "shap_env_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
